1.基线
baseline_ana_R<-function(df_input,
                         outcome,
                         N_features=NULL,
                         Q_features=NULL,
                         method="gtsummary",
                         savepath=NULL,
                         style=1,
                         #theme="nejm",
                         decimal_num=3){

  ### method  gtsummary或者compareGroups

  ### style gtsummary单用
  ### theme=c("jama", "lancet", "nejm", "qjecon") gtsummary单用

  library(rms)
  library(compareGroups)
  library(gtsummary)
  library(gt)
  library(flextable)
  results <- list() #结果
  results$error<-'success'
  table_name<-NULL
  table_path<-NULL
  table_savepath<-NULL
  descrip<-NULL
  head_descrip<-NULL
  str_res<-NULL
  res_scrip<-NULL
  mytime <- format(Sys.time(), "%b_%d_%H_%M_%S_%Y")  #时间
  rand <- sample(1:1000,1)
  head_descrip<-paste0(head_descrip,"\nR版本为：",getRversion())
  head_descrip<-paste0(head_descrip,",所涉及的的主要包版本gtsummary为：",packageVersion('gtsummary'),'\n')

  #features <- c(Q_features, N_features)

  if (method == "gtsummary"){

    #theme_gtsummary_journal(journal=theme)
    if(style!=1){
      compact=TRUE
      eda=TRUE
    }else{
      compact=FALSE
      eda=FALSE
    }
    #打印的表格将更紧凑，字体更小，单元格填充更少
    theme_gtsummary_compact(set_theme=compact)
    #默认显示中值、平均值、IQR、SD和范围
    theme_gtsummary_eda(set_theme=eda)

    if (length(outcome)==1){
      features <- c(Q_features, N_features)
      mydata <- df_input[,c(outcome,features)]

      res_tab<-mydata %>%
        tbl_summary(by=outcome,
                    type = list(Q_features ~ "continuous2"),
                    digits = all_continuous() ~ decimal_num,
                    missing_text = "(Missing)"
        )

        res_tab<-res_tab %>%
          add_p(pvalue_fun = ~ style_pvalue(.x, digits = decimal_num))
        res_tab<-res_tab %>%
          add_overall()
    }
    else{
      features <- c(Q_features, N_features)
      mydata <- df_input[,features]

      res_tab<-mydata %>%
        tbl_summary(type = list(Q_features ~ "continuous2"),
                    digits = all_continuous() ~ decimal_num,
                    missing_text = "(Missing)")
    }

    result = res_tab[["table_body"]]
    result1 = result[,c(1,2,14)]
    result1 = na.omit(result1)
    result1= as.data.frame(result1)
    rownames(result1) = result1$variable

    res_scrip=paste0("利用gtsummary包进行基线分析，研究不同",outcome,"分组中各个指标的差异是否具有统计学意义，总的有效样本为",nrow(df_input),'例，其中')

    for (la in unique(df_input[[outcome]])) {
      count <- sum(df_input[[outcome]] == la, na.rm = TRUE)
      res_scrip <- paste0(res_scrip, outcome, "=", la, "：病例数为", count, "；")
    }

    #paste0(strsplit(result1[i,2], "\\.")[[1]][1:2], collapse = ".")
    coefstat <- NULL
    for(i in 1:nrow(result1)){
      if(result1[i,3]<0.05) {
        coefstat[i] <- paste0(rownames(result1)[i],'经',paste0(strsplit(result1[i,2], "\\.")[[1]][1:2], collapse = "."),"检验p值<0.05,组间存在统计学意义。",'\n')
      }
      else{
        coefstat[i] <- paste0(rownames(result1)[i],'经',paste0(strsplit(result1[i,2], "\\.")[[1]][1:2], collapse = "."),"检验p值>0.05,组间不存在统计学意义。",'\n')
      }
      conclusion <- paste0(res_scrip, paste0(coefstat[1:nrow(result1)],collapse = " "))
    }



    res_tab<-res_tab %>%
      modify_header(label ~ "**Variable**") %>%
      modify_spanning_header(all_stat_cols() ~"**group**") %>%
      modify_footnote(
        all_stat_cols() ~ "Median (IQR) or Frequency (%)"
      ) %>%
      modify_caption("**Baseline Table**")%>%
      bold_labels()
    res_tab1<-res_tab %>% as_gt()
    # rres_tab<-as_tibble(res_tab)
    tab_html=paste0("Table_",mytime,"_",rand,".html")
    #   tab_pdf=paste0("Table_",mytime,"_",rand,".pdf")
    tab_doc=paste0("Table_",mytime,"_",rand,".docx")
    # write.csv(rres_tab,paste0(savepath,tab_name),row.names = FALSE)
    # res_tab %>%
    #   as_gt() %>%
    #   gt::gtsave(filename =paste0(savepath,tab_pdf))
    gtsave(res_tab1,filename =paste0(savepath,tab_html))
    # gtsave(res_tab,filename =paste0(savepath,tab_pdf))
    mytable <- as_flex_table(res_tab)
    save_as_docx(mytable, path = paste0(savepath,tab_doc))
    table_name<-c(table_name,'基线表')
    table_path<-c(table_path,tab_html)
    table_savepath<-c(table_savepath,tab_doc)
  }
  if (method=="compareGroups"){

    if (length(outcome)==1){

      features <- c(Q_features, N_features)
      mydata <- df_input[,c(outcome,features)]

      mydata[N_features] <- lapply(mydata[N_features], as.factor)

      formula <- paste0(outcome,'~',paste(features,collapse = '+'))

      if (decimal_num==3){
        res = descrTable( as.formula(formula), data = mydata,include.miss = TRUE,
                          lab.missing=TRUE,show.p.overall=TRUE,
                          method = NA,show.all=TRUE,digits=3,extra.labels=c("Mean (SD)","Median [Q1-Q3]", "N (%)" ))####.符号代表包括其他的变量
      }

      if (decimal_num==2){
          res = descrTable( as.formula(formula), data = mydata,include.miss = TRUE,
                            lab.missing=TRUE,show.p.overall=TRUE,
                            method = NA,show.all=TRUE,digits=2,extra.labels=c("Mean (SD)","Median [Q1-Q3]", "N (%)" ))####.符号代表包括其他的变量
      }
      if (decimal_num==1){
          res = descrTable( as.formula(formula), data = mydata,include.miss = TRUE,
                            lab.missing=TRUE,show.p.overall=TRUE,
                            method = NA,show.all=TRUE,digits=1,extra.labels=c("Mean (SD)","Median [Q1-Q3]", "N (%)" ))####.符号代表包括其他的变量
      }
    }else{
      features <- c(Q_features, N_features)
      mydata <- df_input[,features]

      mydata[N_features] <- lapply(mydata[N_features], as.factor)

      formula <- paste0('~',paste(features,collapse = '+'))

      res = descrTable(as.formula(formula), data = mydata,method = NA,
                       show.all=TRUE,digits=1,extra.labels=c("Mean (SD)","Median [Q1-Q3]", "N (%)" ))####.符号代表包括其他的变量
    }

    res_scrip <- paste0(res_scrip,"[","基线表","]如基线表所示，compareGroups包在统计检验时，对于数值变量，分组变量是二分类的默认使用Student‘s t检验（t-test），分组变量是多分类的默认使用方差分析（ANOVA）。")
    res_scrip <-  paste0(res_scrip,"\n对于分类变量，无论分组变量是二分类还是多分类，默认都使用卡方检验（Chi-squared test），当列联表中存在期望频数 < 5 的格子数超过 20%，或任一期望频数 < 1 时，软件会自动采用Fisher精确检验（Fisher‘s exact test）。")

    conclusion <- paste0(res_scrip)

    tab_html=paste0("Table_comparegroup_",mytime,"_",rand,".xlsx")
    export2xls(res, file =paste0(savepath,tab_html))
    table_name<-c(table_name,'基线表')
    table_path<-c(table_path,tab_html)
    table_savepath<-c(table_savepath,tab_html)
  }


  descrip<-paste0(descrip,conclusion,head_descrip)
  results$table_name<-table_name
  results$table_path<-table_path
  results$table_savepath<-table_savepath
  results$descrip<-enc2utf8(descrip)
  return(results)
}







2.数据清洗部分

2.1  ai数据填补
import datetime
import os
import traceback # 新增：用于捕获详细的异常信息
#解耦导入
from AnalysisFunction.celerytest.utils.data_describe import _describe
from AnalysisFunction.celerytest.utils.feature_classification import _feature_classification
from AnalysisFunction.celerytest.utils.get_environment import _get_environment_info
from AnalysisFunction.celerytest.utils.knn_imputation import _knn_imputation
from AnalysisFunction.celerytest.utils.rf_imputation import _rf_imputation


current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('AGG')
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
import datetime
import random
import warnings
import matplotlib as mpl
mpl.rcParams['axes.grid'] = False
plt.rcParams['font.family'] = 'sans-serif'
warnings.filterwarnings('ignore')

# 新增：获取环境信息
env_info = _get_environment_info(__file__)

imputation_method = {'mean': '均值', 'median': '中位数', 'most_frequent': '众数',
                     'nan': '空值', 'randomforest': '随机森林', 'interpolate': '插值法', 'mul_interpolate': '多重插补', 'KNN': 'KNN'}

dict_method = dict(imputation_method)


"""
智能数据填补
df_input:Dataframe 处理数据
features:list 填补特征(列)
method:str 填补方式(一般数据填补：'mean': '均值', 'median': '中位数', 'most_frequent': '众数',constant:‘常数填补',
                 nan': '空值'，'interpolate': '插值法','mul_interpolate': '多重插补',
                  AI智能填补：'randomforest': '随机森林填补', 'KNN': 'KNN')
constant：object 常数填补参数
output_dir：图片保存路径
dpi：图片保存分辨率
image_format：图片另存格式
"""


def data_filling(df_input, features=None, method=None, constant=None, output_dir=None, dpi=600, image_format='jpeg'):
    try:
        # tables_list = []
        # figures_list = [] # 由 distribution_curve 填充

        if features is None:
            continuous_features, categorical_features, time_features = _feature_classification(df_input)
            features = continuous_features
            df_temp = df_input[features]
        else:
            df_temp = df_input[features]

        if (method == 'randomforest'):
            df_temp = _rf_imputation(df_temp)
            method_str = dict_method[method]

        elif method in ["mean", "median", "most_frequent"]:
            imp_ = SimpleImputer(missing_values=np.nan, strategy=method)
            df_temp = pd.DataFrame(data=imp_.fit_transform(df_temp), columns=features)
            method_str = dict_method[method]
        elif method == 'constant':
            imp_ = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=constant)
            df_temp = pd.DataFrame(data=imp_.fit_transform(df_temp), columns=features)
            method_str = '常数' + str(constant)
        elif method == 'interpolate':
            df_temp = df_temp.interpolate().fillna(method='bfill')  # 插值法不能对首行空值填充，后面加上向后填充方法
            method_str = dict_method[method]
        elif method =='mul_interpolate':
            from AnalysisFunction.X_5_R_SmartPlot import mul_interpolate_R_docker
            mul_str_result, df_temp = mul_interpolate_R_docker(df_temp, method='pmm', m=10, path=output_dir)
            if mul_str_result == "":
                method_str = dict_method[method]
                df_temp = pd.DataFrame(df_temp)
            else:
                raise ValueError(mul_str_result)

        elif method == 'KNN':
            df_temp = _knn_imputation(df_temp)
            method_str = dict_method[method]
        else:
            raise ValueError('请输入正确的方法')

        # 图片监控
        def distribution_curve(df_pre, df_pos, features, path, dpi=600, picFormat='jpeg'):
            figures_list = [] # 初始化
            for feature in features:
                str_time = str(datetime.datetime.now().hour) + str(datetime.datetime.now().minute) + str(
                    datetime.datetime.now().second)
                random_number = random.randint(1, 1000)
                str_time = str_time + str(random_number)
                
                plot_base_name = 'distribution_curve_' + feature.replace(' ', '_') + '_' + str_time
                pic_name_png = plot_base_name + '.png'
                pic_name_jpeg = plot_base_name + '.' + picFormat
                plt.figure(figsize=[6, 6],dpi=dpi)
                
                try:
                    temp1 = df_pre[feature]
                    temp2 = df_pos[feature]
                    temp1.plot(kind='kde', label='pre')
                    temp2.plot(kind='kde', label='post')
                    plt.title(feature)
                    plt.legend()
                    plt.savefig(os.path.join(path, pic_name_png), bbox_inches='tight', format='png')
                    plt.savefig(os.path.join(path, pic_name_jpeg), bbox_inches='tight',format=picFormat)

                    figures_list.append({
                        "name": f"分布曲线_{feature}",
                        "file": {"png": pic_name_png, picFormat: pic_name_jpeg},
                        "description": f"'{feature}' 填补前 (pre) 与填补后 (post) 的分布曲线。"
                    })

                except Exception as e:
                    figures_list.append({
                        "name": f"分布曲线_{feature}",
                        "file": {},
                        "description": f"图形生成失败: {str(e)}"
                    })
                finally:
                    plt.close('all') 
            
            return figures_list # 返回标准格式的列表

        df_result = df_input.copy()
        df_result = df_result.drop(features, axis=1)
        df_result.index = list(df_temp.index)
        
        if len(df_temp[features])!=len(df_result):
            raise ValueError("填补的数据中存在共有缺失的数据使得填补之后数据总样本小于整体样本量，请再添加特征重新填补！")
        
        df_result = pd.concat([df_result, df_temp[features]], axis=1)
        figures_list = distribution_curve(df_input, df_result, features, output_dir, dpi=dpi, picFormat=image_format)

        str_result = '采用' + method_str + '填补的方式对' + "、".join(features) + '进行数据填补'
        input_o_desc, input_n_desc = _describe(df_input[features])
        result_o_desc, result_n_desc = _describe(df_result[features])

        input_o_desc.fillna('', inplace=True)
        input_n_desc.fillna('', inplace=True)
        result_o_desc.fillna('', inplace=True)
        result_n_desc.fillna('', inplace=True)

        tables_list = [
            {
                "name": "填补后数据表",
                "file": {"dataframe": df_result},
                "description": "包含填补后特征的完整数据集。"
            },
            {
                "name": "原分类变量描述表",
                "file": {"dataframe": input_o_desc},
                "description": "填补前分类变量的描述性统计。"
            },
            {
                "name": "原连续变量描述表",
                "file": {"dataframe": input_n_desc},
                "description": "填补前连续变量的描述性统计。"
            },
            {
                "name": "分析之后分类变量描述表",
                "file": {"dataframe": result_o_desc},
                "description": "填补后分类变量的描述性统计。"
            },
            {
                "name": "分析之后连续变量描述表",
                "file": {"dataframe": result_n_desc},
                "description": "填补后连续变量的描述性统计。"
            }
        ]
        return {
            "status": "success",
            "description": str_result,
            "tables": tables_list,
            "figures": figures_list,
            "environment": env_info
        }

    except ValueError as e:
        return {
            "status": "error",
            "description": str(e),
            "tables": [],
            "figures": [],
            "environment": env_info
        }
    
    # 优化：捕获意外的运行时错误
    except Exception as e:
        error_message = traceback.format_exc()
        print(error_message) # 打印追溯信息
        return {
            "status": "exception",
            "description": f"发生意外错误: {str(e)}\n\nTraceback:\n{error_message}",
            "tables": [],
            "figures": [],
            "environment": env_info
        }


2.2 异常值处理
def abnormal_deviation_process(df_input, features, method='median', ratio=1.5, path=None,dpi=600,picFormat='jpeg'):
    df_feature = df_input[features]

    gl_res_str_adp = ""
    gl_index_adp=0
    # df_feature1 = df_feature.copy()
    def lower_upper_limit(c):
        lower_q = np.nanquantile(c, 0.25, interpolation='lower')  # 下四分位数
        higher_q = np.nanquantile(c, 0.75, interpolation='higher')  # 上四分位数
        int_r = higher_q - lower_q
        lower_limit = lower_q - ratio * int_r
        high_limit = higher_q + ratio * int_r
        return lower_limit, high_limit

    def set_Disposition(columns, Disposition):
        if Disposition == 'median':
            a = np.median(columns)
        elif Disposition == 'mean':
            a = np.mean(columns)
        elif Disposition == 'most_frequent':
            a = Counter(columns).most_common()[0][0]
        elif Disposition == 'zero':
            a = 0
        elif Disposition == 'nan':
            a = np.nan
        return a

    def replace_exception(column_list):
        # 将列中异常值进行替换
        nonlocal gl_res_str_adp
        nonlocal gl_index_adp
        column_list1 = column_list.copy()
        gl_res_str_adp += "对变量"+features[gl_index_adp]+"进行处理："
        lower_limit, high_limit = lower_upper_limit(column_list)
        re_str=""
        for i in range(len(column_list)):
            if i==0:
                gl_index_adp+=1
            if column_list[i] < lower_limit or column_list[i] > high_limit:
                column_list1[i] = set_Disposition(column_list, method)
                re_str+=str(i+1)+","
        if re_str!="":
            gl_res_str_adp += re_str+"行的数据存在偏离\n"
        else:
            gl_res_str_adp += "没有偏离的数据\n"
        return column_list1
    df_temp = df_feature.apply(replace_exception, axis=0)
    str_result = '采用' + dict_method[method] + '方法对' + "、".join(features) + '中异常偏离超过正常值范围' + str(ratio) + '倍值的数据进行处理。'
    df_result = df_input.drop(features, 1)
    df_result = pd.concat([df_result, df_temp], axis=1)
    input_o_desc, input_n_desc = _describe(df_input[features])
    result_o_desc, result_n_desc = _describe(df_result[features])
    input_o_desc.fillna('', inplace=True)
    input_n_desc.fillna('', inplace=True)
    result_o_desc.fillna('', inplace=True)
    result_n_desc.fillna('', inplace=True)
    ax1 = plt.figure()
    plt_dict_path = {}
    plt_dict_path_save = {}
    if len(features)<19:
        plt01 = x5.comparison_plot(df_input=df_feature, features=features, group=None, kind='box', concat_way='free',
                                   row_size=1, col_size=len(features), path=path, dpi=dpi, picFormat=picFormat)
        ax2 = plt.figure()
        plt02 = x5.comparison_plot(df_input=df_temp, features=features, group=None, kind='box', concat_way='free',
                                   row_size=1,
                                   col_size=len(features), path=path, dpi=dpi, picFormat=picFormat)

        plt_dict_path.update({'比较图1': plt01['pics']['比较图']})
        plt_dict_path_save.update({'比较图1': plt01['save_pics']['比较图']})
        plt_dict_path.update({'比较图2': plt02['pics']['比较图']})
        plt_dict_path_save.update({'比较图1': plt02['save_pics']['比较图']})
    # if len(features) == 1:
    #     df_box = pd.concat([df_feature, df_temp], axis=1)
    #     df_box.columns = [features[0], features[0] + "(处理后)"]
    #     df_box.boxplot(  # 指定绘图数据
    #                 patch_artist=True,  # 要求用自定义颜色填充盒形图，默认白色填充
    #                 showmeans=True,  # 以点的形式显示均值
    #                 boxprops={'color': 'black', 'facecolor': 'steelblue'},  # 设置箱体属性，如边框色和填充色
    #                 flierprops={'marker': 'o', 'markerfacecolor': 'red', 'markersize': 3},
    #                 # 设置均值点的属性，如点的形状、填充色和点的大小
    #                 meanprops={'marker': 'D', 'markerfacecolor': 'indianred', 'markersize': 4},
    #                 # 设置中位数线的属性，如线的类型和颜色
    #                 medianprops={'linestyle': '--', 'color': 'orange'},
    #                 # labels=['']  # 删除x轴的刻度标签，否则图形显示刻度标签为1
    #                 )
    result_dict = {'str_result': {'分析结果描述': str_result+gl_res_str_adp},
                   'tables': {'最终数据': df_result, '原分类变量描述表': input_o_desc, '原连续变量描述表': input_n_desc,
                              '分析之后分类变量描述表': result_o_desc, '分析之后连续变量描述表': result_n_desc, },
                   'pics': plt_dict_path, 'save_pics': plt_dict_path_save}
    return result_dict


2.3 2.4 自动剔除（行，列）
def miss_data_delete(df_input, miss_rate, miss_axis,features=None):
    str_result = '无缺失率大于' + str(miss_rate) + '的数据'
    list1=[]
    if (miss_axis == 1):
        miss_rowtotal, miss_rowper = _miss_row(df_input)
        list1 = miss_rowper[miss_rowper > miss_rate].index
        df_result = df_input.drop(list1)
        # df_miss_rate = pd.concat([df_input, miss_rowper], axis=1)
        if len(list1) > 0:
            list_new = map(lambda x: str(x+2), list1)
            #str_result = '剔除缺失率大于' + str(miss_rate) + '的案例，剔除案例的id为' + "、".join(list_new)
            str_result = '剔除缺失率大于' + str(miss_rate) + '的案例，剔除剔除的行号为' + "、".join(list_new)
    if (miss_axis == 0):
        col_total, col_percent = _miss_col(df_input)
        list1 = col_percent[col_percent > miss_rate].index
        df_result = df_input.drop(list1, axis=1)
        # df_miss_rate = df_input.append(col_percent, ignore_index=True)
        if len(list1) > 0:
            list_new = map(lambda x: str(x), list1)
            str_result = '剔除缺失率大于' + str(miss_rate) + '的变量，共剔除' + str(len(list1)) + '个变量,是' + "、".join(list_new)
            for i, v in col_percent.items():
                if v <= miss_rate:
                    break
                else:
                    str_result += '\n%s缺失率为%.2f%%,共缺失样本%d例' % (
                        i, (v * 100), col_total[i]
                    )
    if not features is None:
        if len(list1)>0:
            ff=list(set(features)-set(list1))
        else:
            ff=features
        input_o_desc, input_n_desc = _describe(df_input[ff])
        result_o_desc, result_n_desc = _describe(df_result[ff])
    else:
        input_o_desc, input_n_desc = _describe(df_input)
        result_o_desc, result_n_desc = _describe(df_result)


    input_o_desc.fillna('', inplace=True)
    input_n_desc.fillna('', inplace=True)
    result_o_desc.fillna('', inplace=True)
    result_n_desc.fillna('', inplace=True)

    df_result = df_result.reset_index(drop=True)


    result_dict = {'str_result': {'分析结果描述': str_result},
                   'tables': {'最终数据': df_result,'原分类变量描述表':input_o_desc,'原连续变量描述表':input_n_desc,'分析之后分类变量描述表':result_o_desc,'分析之后连续变量描述表':result_n_desc,},
                   'pics': None}
    return result_dict


3.样本重采样
def data_balance(df_input, group, ratio, method='SMOTE', randomState=42):
    from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler, BorderlineSMOTE
    from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler
    from imblearn.combine import SMOTEENN, SMOTETomek
    from sklearn.cluster import KMeans
    from sklearn.utils import resample
    plot_name_dict_save = {}  ##存储图片
    df_input = df_input.dropna(subset=[group])
    Ratio = ratio
    list_name = [group]
    continuous_features, categorical_features, time_features = _feature_classification(df_input)
    features = continuous_features
    if group not in features:
        features += group
    df_temp = df_input[features].copy()
    df_temp = df_temp.fillna(0)
    Y = df_temp.loc[:, list_name]
    X = df_temp.drop(group, axis=1)
    column_nameX = X.columns

    try:
        if method in ['SMOTE', 'ADASYN', 'RandomOverSampler', 'BorderlineSMOTE']:
            if method == 'SMOTE':
                sampler = SMOTE(sampling_strategy=Ratio, random_state=randomState)
            elif method == 'ADASYN':
                sampler = ADASYN(sampling_strategy=Ratio, random_state=randomState)
            elif method == 'RandomOverSampler':
                sampler = RandomOverSampler(sampling_strategy=Ratio, random_state=randomState)
            elif method == 'BorderlineSMOTE':
                sampler = BorderlineSMOTE(sampling_strategy=Ratio, kind='borderline-1', random_state=randomState)

            X_resampled, Y_resampled = sampler.fit_resample(X, Y)

        elif method in ['RandomUnderSampler', 'ClusterCentroids', 'NearMiss']:
            if method == 'RandomUnderSampler':
                sampler = RandomUnderSampler(sampling_strategy=Ratio, random_state=randomState)
            elif method == 'ClusterCentroids':
                sampler = ClusterCentroids(sampling_strategy=Ratio, random_state=randomState)
            elif method == 'NearMiss':
                sampler = NearMiss(sampling_strategy=Ratio)

            X_resampled, Y_resampled = sampler.fit_resample(X, Y)

        elif method in ['SMOTEENN', 'SMOTETomek']:
            if method == 'SMOTEENN':
                sampler = SMOTEENN(sampling_strategy=Ratio, random_state=randomState)
            elif method == 'SMOTETomek':
                sampler = SMOTETomek(sampling_strategy=Ratio, random_state=randomState)

            X_resampled, Y_resampled = sampler.fit_resample(X, Y)

            # # 调整样本数量，确保总样本数与原始数据一致
            # total_samples = len(X)
            # if len(X_resampled) < total_samples:
            #     # 从重采样的数据中有放回地抽样，增加样本数量
            #     X_resampled, Y_resampled = resample(
            #         X_resampled, Y_resampled,
            #         replace=True,
            #         n_samples=total_samples,
            #         random_state=randomState
            #     )
            # elif len(X_resampled) > total_samples:
            #     # 随机下采样，减少样本数量
            #     X_resampled, Y_resampled = resample(
            #         X_resampled, Y_resampled,
            #         replace=False,
            #         n_samples=total_samples,
            #         random_state=randomState
            #     )

        elif method == 'BDSK':
            bsmote = BorderlineSMOTE(sampling_strategy=ratio, random_state=randomState)
            X_res, Y_res = bsmote.fit_resample(X, Y)

            # 检查过采样是否增加了数据集大小。如果是，则进行下采样。改进的下采样。
            if len(X_res) != len(X):
                rus = RandomUnderSampler(sampling_strategy=ratio, random_state=randomState)  # 使用 RandomUnderSampler
                X_resampled, Y_resampled = rus.fit_resample(X_res, Y_res)
            else:
                X_resampled = X_res
                Y_resampled = Y_res


        elif method == 'BalanceCascade':
            from imblearn.ensemble import RUSBoostClassifier
            from sklearn.linear_model import LogisticRegression
            balance_cascade = RUSBoostClassifier(sampling_strategy=ratio, random_state=randomState,n_estimators=10)
            balance_cascade.fit(X, Y.values.ravel())
            # 使用模型的预测结果模拟采样
            Y_pred = balance_cascade.predict(X)  # 获取预测标签
            X_resampled = X  # 原数据保持不变
            Y_resampled = Y.copy()
            Y_resampled.loc[:, group] = Y_pred



        elif method == 'EasyEnsemble':
            from imblearn.ensemble import EasyEnsembleClassifier  # 使用分类器
            easy_ensemble = EasyEnsembleClassifier(sampling_strategy=ratio,n_estimators=10, random_state=randomState)
            easy_ensemble.fit(X, Y.values.ravel())  # 训练分类器

            # 使用模型的预测结果模拟采样
            Y_pred = easy_ensemble.predict(X)  # 获取预测标签
            X_resampled = X  # 原数据保持不变
            Y_resampled = Y.copy()
            Y_resampled.loc[:, group] = Y_pred


        # 对于未包含的情况，返回错误
        else:
            return {'error': '不支持的方法：' + method + ' false-error'}

    except Exception as e:
        return {'error': str(e) + ' false-error'}

    # 将X和Y合并
    array_result = np.column_stack((X_resampled, Y_resampled))
    list_column_name = list(column_nameX)
    list_column_name.append(group)
    df_result = pd.DataFrame(array_result, columns=list_column_name)

    # 确保数据类型一致
    df_result = df_result.astype(df_temp.dtypes.to_dict())
    # if not keep_decimals:
    #     df_result = df_result.round(0)

    group_labels = np.unique(df_result[group])
    str_temp = ''
    for group_label in group_labels:
        str_temp += group + '(' + str(group_label) + ')=' + str((df_result[group] == group_label).sum()) + '例，'

    str_result = '采用' + dict_method[method] + '方法对数据进行平衡，使得少数类和多数类的比例为' + str(Ratio) + \
                 '，最终匹配结果为' + str_temp + \
                 '该方法会自动剔除包含空值的行，以及非数值变量的列。'


    input_o_desc, input_n_desc = _describe(df_input)
    result_o_desc, result_n_desc = _describe(df_result)
    input_o_desc.fillna('', inplace=True)
    input_n_desc.fillna('', inplace=True)
    result_o_desc.fillna('', inplace=True)
    result_n_desc.fillna('', inplace=True)
    result_dict = {'str_result': {'分析结果描述': str_result},
                   'tables': {'最终数据': df_result, '原分类变量描述表': input_o_desc, '原连续变量描述表': input_n_desc,
                              '分析之后分类变量描述表': result_o_desc, '分析之后连续变量描述表': result_n_desc, },
                   }
    # output_path = os.path.join(savePath, 'balanced_data.csv')
    # df_result.to_csv(output_path, index=False)
    return result_dict


4.共线性分析
def get_var_vif(df_input, features=None,decimal_num=3):
    if features is not None:
        df_input = df_input[features]
    else:
        n,b=_feature_get_n_b(df_input)
        df_input = df_input[n+b]
    df_input=df_input.dropna()
    df_input[df_input.shape[1]]=1
    #vif
    vif=[]
    for i in range(df_input.shape[1] - 1):
        vif.append(variance_inflation_factor(df_input.values, i))
    #result_out
    df_result=pd.DataFrame(df_input.columns[:-1, ])
    df_result.rename(columns={0:"变量名"},inplace=True)
    df_result["vif"]=vif
    df_result[["vif"]]= df_result[["vif"]].applymap(lambda x: _round_dec(x, decimal_num))
    df_result=df_result.sort_values(['vif'],ascending = False)
    df_result.reset_index(drop = True,inplace=True)

    result_dict = {'str_result': None,
                   'tables': {'分析结果描述表': df_result},
                   'pics': None}
    return result_dict

5.pearson
def get_var_correlation(df_input, independent_variable=None, cor_method='pearson', plot_method='heatmap', hue_=None,
                        decimal_places=3, output_dir=None, cmap_style=None, annot=True,dpi=600, image_format='jpeg'):
    """
    相关性分析
    df_input: 输入的数据框（DataFrame），包含要分析的特征。
    independent_variable: 要分析的特征列表。如果为None，则分析数据集中的所有数值型特征。
    cor_method: 相关性计算方法，默认为'pearson'，可选'kendall'或'spearman'。
    plot_method: 可视化方法，默认为'heatmap'，可选'clustermap'或'pairplot'。
    hue_: 用于分组的颜色变量，仅在plot_method为'pairplot'时使用。
    decimal_places: 相关性系数和P值保留的小数位数，默认为3。(原 decimal_num)
    output_dir: 保存图片的路径。(原 savepath)
    cmap_style: 热图的颜色映射风格。
    # annot: 是否在热图上显示相关性系数，默认为False。
    dpi: 图片的分辨率，默认为600。
    image_format: 保存图片的格式，默认为'jpeg'。(原 picFormat)

    返回:
    result_dict: 包含相关性分析结果的字典，遵循标准输出格式。
    """
    # 初始化返回结果
    figures_list = []
    tables_list = []
    if output_dir and not os.path.exists(output_dir):
        try:
            os.makedirs(output_dir)
        except OSError as e:
            return {
                "status": "exception",
                "description": f"无法创建保存路径 '{output_dir}'。请检查路径权限或设置：{str(e)}",
                "tables": [],
                "figures": [],
                "environment": env_info
            }

    try:
        if independent_variable is not None:
            # 检查所有指定特征是否存在
            missing_features = [f for f in independent_variable if f not in df_input.columns]
            if missing_features:
                raise ValueError(f"错误：输入数据中找不到以下变量: {', '.join(missing_features)}。")
            df_temp = df_input[independent_variable]
        else:
            # 如果未指定
            df_temp = df_input.select_dtypes(include=['number'])
            independent_variable = df_temp.columns.tolist()
            if not independent_variable:
                raise ValueError("错误：数据集中未找到任何数值型特征，且未指定 'features' 参数。")

        # 检查是否存在非数值型的分类数据
        _, cc, _ = _feature_classification(df_temp[independent_variable])
        if len(cc) > 0:
            raise ValueError(f"分析中存在非数值型的分类数据{cc}，请先将其转化为数值型的数据之后再进行分析！")

        # 根据特征数量设置注释的大小
        if len(independent_variable) < 10:
            annot_kws = {'size': 8}
        elif 10 <= len(independent_variable) < 15:
            annot_kws = {'size': 6}
        else:
            annot_kws = {'size': 4}

        # 去除缺失值
        df_temp = df_temp.dropna()

        # 检查数据点数量 (新增校验)
        if len(df_temp) < 3:
            raise ValueError(f"经过缺失值处理后，剩余数据点少于3个 (实际: {len(df_temp)}个)，无法进行相关性分析。")

        # 计算相关性
        df_result = df_temp.corr(method=cor_method) # df_result 是相关性矩阵
        print("111111111")
        print(df_result.shape)

        rrr, ppp = [], []
        # 计算每对特征之间的相关性系数和P值
        for feature1 in independent_variable:
            rr, pp = [], []
            for feature2 in independent_variable:
                if cor_method == 'pearson':
                    r, p = stats.pearsonr(df_temp[feature1], df_temp[feature2])
                elif cor_method == 'kendall':
                    r, p = stats.kendalltau(df_temp[feature1], df_temp[feature2])
                elif cor_method == 'spearman':
                    r, p = stats.spearmanr(df_temp[feature1], df_temp[feature2])
                rr.append(r)
                pp.append(p)
            rrr.append(rr)
            ppp.append(pp)

        #格式化P值表
        df_result_p = pd.DataFrame(ppp, columns=independent_variable, index=independent_variable)
        df_result_p_formatted = df_result_p.applymap(
            lambda p: '<0.001' if isinstance(p, (int, float)) and 0 < p < 0.001 else (
                '0' if p == 0 else round_dec_pvalue(p, decimal_places)
            )
        )
        df_result_p_formatted.reset_index(inplace=True)
        df_result_p_formatted.rename(columns={'index': '变量'}, inplace=True)

        tables_list.append({
            "name": f"{cor_method.capitalize()} 相关性 P 值表",
            "file": {"dataframe": df_result_p_formatted},
            "description": f"展示了变量间的 {cor_method} 相关性检验的 p 值。p < 0.05 通常被认为具有统计显著性。"
        })
    #结果描述
        conclusion_str = ""
        for i in range(len(independent_variable)):
            for j in range(i + 1, len(independent_variable)):
                feature1, feature2 = independent_variable[i], independent_variable[j]
                r, p = rrr[i][j], ppp[i][j]
                # 使用原始的 P 值 (ppp) 进行判断
                if p < 0.05:  # 假设显著性水平为0.05
                    if abs(r) > 0.5:  # 假设相关系数的绝对值大于0.5表示强相关
                        conclusion_str += f"{feature1} 和 {feature2} 之间存在显著的强相关性（相关系数：{r:.3f}，P值：{p:.3f}）。\n"
                    else:
                        conclusion_str += f"{feature1} 和 {feature2} 之间存在显著的相关性（相关系数：{r:.3f}，P值：{p:.3f}）。\n"
                else:
                    conclusion_str += f"{feature1} 和 {feature2} 之间没有显著的相关性（相关系数：{r:.3f}，P值：{p:.3f}）。\n"

        str_result = f"使用的算法: {cor_method}相关性分析\n"
        str_result += f"分析的变量: {', '.join(independent_variable) if independent_variable else '所有数值型特征'}\n"
        str_result += f"分析结果：{conclusion_str}\n"
        str_result += f"分析描述图表说明：相关系数的取值范围是-1到1。其中，-1表示完全负相关，1表示完全正相关，0表示无相关性。通常根据相关系数的绝对值大小来判断变量之间的相关强度，一般大于0.5" \
                      f"说明变量之间的相关性比较高。\n "
        str_result += f"相关性P值图表说明：相关系数的p值用于衡量统计显著性。p值越小，表示相关系数不为0的证据越强，通常p值小于0.05" \
                      f"被认为是统计显著的。若呈现显著性，则说明两变量之间存在相关性，反之，则两变量之间不存在相关性。\n "

        # 生成图表
        # 定义文件名
        plot_name = f"{cor_method}_{plot_method}"
        plot_name_png = f"{plot_name}.png"
        plot_name_format = f"{plot_name}.{image_format}"

        # 用于 figure_list 的描述
        plot_desc_for_list = ""

        try:
            if plot_method == 'heatmap':
                plt.figure(figsize=(6, 5), dpi=dpi)
                ax = plt.gca()
                sns.heatmap(df_result, linewidths=0.3, cmap=cmap_style,annot=True,annot_kws=annot_kws, ax=ax)

                print("2222222222222")
                annot_matrix=np.round(df_result,2)
                print(annot_matrix.shape)

                str_result += "图表说明：以热力图的形式展示了相关系数的值，主要通过颜色深浅去表示值的大小，颜色越深数值越小，颜色越浅数值越大。"
                plot_desc_for_list = "以热力图的形式展示了相关系数的值，主要通过颜色深浅去表示值的大小，颜色越深数值越小，颜色越浅数值越大。"


                # 保存 Heatmap
                if output_dir:
                    plt.savefig(os.path.join(output_dir, plot_name_format), bbox_inches="tight", dpi=dpi, format=image_format)
                    plt.savefig(os.path.join(output_dir, plot_name_png), bbox_inches="tight", dpi=dpi, format='png')
                plt.close()

            elif plot_method == 'clustermap':
                # g = sns.clustermap(df_result, linewidths=0.3, cmap=cmap_style, annot=annot, annot_kws=annot_kws, figsize=(7, 6))

                g = sns.clustermap(df_result, linewidths=0.3, cmap=cmap_style, annot=True,annot_kws=annot_kws, figsize=(7, 6))
                str_result += "图表说明：将数据集中的样本按照相似性进行聚类，并将聚类结果以矩阵的形式展示出来。通过颜色深浅来表示数据值的大小或类别，从而直观地展示数据间的相似性和差异性。"
                plot_desc_for_list = "将数据集中的样本按照相似性进行聚类，并将聚类结果以矩阵的形式展示出来。通过颜色深浅来表示数据值的大小或类别，从而直观地展示数据间的相似性和差异性。"

                # 保存 Clustermap
                if output_dir:
                    g.savefig(os.path.join(output_dir, plot_name_format), bbox_inches="tight", dpi=dpi, format=image_format)
                    g.savefig(os.path.join(output_dir, plot_name_png), bbox_inches="tight", dpi=dpi, format='png')
                plt.close(g.fig)

            elif plot_method == 'pairplot':
                plot_features = independent_variable.copy()
                if hue_:
                    if hue_ not in df_input.columns:
                        raise ValueError(f"用于分组的 'hue_' 变量 '{hue_}' 在数据中不存在。")
                    if hue_ not in plot_features:
                        plot_features.append(hue_)
                else:
                    # 确保 hue_ 为 None
                    hue_ = None

                if hue_ and hue_ not in independent_variable:
                     independent_variable.append(hue_)
                     g = sns.pairplot(df_input[independent_variable], hue=hue_)
                     independent_variable.pop() # 恢复 features 列表
                else:
                     g = sns.pairplot(df_input[independent_variable], hue=hue_)

                str_result += "图表说明：创建一个矩阵的成对关系图，显示数据集中所有变量的成对关系,它自动为每对变量生成一个散点图，并且可以使用直方图或核密度估计（KDE）来显示每个变量的单变量分布。"
                plot_desc_for_list = "创建一个矩阵的成对关系图，显示数据集中所有变量的成对关系,它自动为每对变量生成一个散点图，并且可以使用直方图或核密度估计（KDE）来显示每个变量的单变量分布。"

                # 保存 Pairplot
                if output_dir:
                    g.savefig(os.path.join(output_dir, plot_name_format), bbox_inches="tight", dpi=dpi, format=image_format)
                    g.savefig(os.path.join(output_dir, plot_name_png), bbox_inches="tight", dpi=dpi, format='png')
                plt.close(g.fig)

            # 记录图表信息
            figures_list.append({
                "name": f"相关性分析图 ({plot_method.capitalize()})",
                "file": {"png": plot_name_png, image_format: plot_name_format},
                "description": plot_desc_for_list
            })

        except Exception as e:
            plt.close('all')  # 关闭所有可能打开的图
            figures_list.append({
                "name": f"相关性分析图 ({plot_method.capitalize()})",
                "file": {},
                "description": f"图表生成失败: {str(e)}"
            })

            if plot_method == 'heatmap':
                 str_result += "图表说明：以热力图的形式展示了相关系数的值，主要通过颜色深浅去表示值的大小，颜色越深数值越小，颜色越浅数值越大。"
            elif plot_method == 'clustermap':
                 str_result += "图表说明：将数据集中的样本按照相似性进行聚类，并将聚类结果以矩阵的形式展示出来。通过颜色深浅来表示数据值的大小或类别，从而直观地展示数据间的相似性和差异性。"
            elif plot_method == 'pairplot':
                 str_result += "图表说明：创建一个矩阵的成对关系图，显示数据集中所有变量的成对关系,它自动为每对变量生成一个散点图，并且可以使用直方图或核密度估计（KDE）来显示每个变量的单变量分布。"
            str_result += f" (图表生成失败: {str(e)})\n"


        df_result = df_result.applymap(lambda x: round_dec_pvalue(x, decimal_places))
        df_result.reset_index(inplace=True)
        df_result.rename(columns={'index': '变量'}, inplace=True)

        tables_list.append({
            "name": "分析结果描述表",
            "file": {"dataframe": df_result},
            "description": f"展示了变量间的 {cor_method} 相关性系数值。取值范围从 -1 (完全负相关) 到 1 (完全正相关)。"
        })


        # 最终返回字典
        result_dict = {
            "status": "success",
            "description": str_result,
            "tables": tables_list,
            "figures": figures_list,
            "environment": env_info
        }
        return result_dict

    except ValueError as e:
        return {
            "status": "error",
            "description": str(e),
            "tables": [],
            "figures": [],
            "environment": env_info
        }

    except Exception as e:
        # 捕获任何其他意外的运行时错误
        error_message = traceback.format_exc()
        return {
            "status": "exception",
            "description": f"发生意外错误: {str(e)}\n\nTraceback:\n{error_message}",
            "tables": [],
            "figures": [],
            "environment": env_info
        }


6.lasso
def R_lasso(df_input, dependent_variable, feature, tim="", savePath=None, method='binomial', decimal_num=3, dpi=600,
            picFormat='jpeg'):
    """
    LASSO回归
    df_input：DataFrame 输入的待处理数据
    dependent_variable：str 应变量
    feature: strvector 自变量集合
    adjust: str 调整变量
    method:str 分类还是连续因变量?binomial(分类),gaussian(连续)
    savePath:str 图片路径
    decimal_num: int  小数点位数
    """
    LASSO = ro.r(
        '''
#V1.0.0
#Author YUANKE
#date 2021年8月18日11:40:35
LASSO <- function(mydata,target,feature,tim=NULL,fml="binomial",savePath,P=0.05,round= 3,dpi=600,picFormat='jpeg'){
  # input:
  # mydata:dataframe 需处理的数据：要求数据框只能是数值型，不能有字母汉字
  # target:str 应变量
  # feature: strvector 自变量集合
  # fml: str 分类还是连续因变量?binomial(分类),gaussian(连续)
  # savePath:str 图片保存路径
  # round:int 小数点位数
  # return:
  # results$p1: str 图1
  # results$p2: str 图2
  # results$descrip: str 描述结果
  library(glmnet)
  library(survival)
  set.seed(1234)
  result<-data.frame()
  results <- list() #结果
  descrip<-NULL
  mytime <- format(Sys.time(), "%b_%d_%H_%M_%S_%Y")  #时间
  rand <- sample(1:100,1)
  mydata <- na.omit(mydata)
  mydata<-as.data.frame(lapply(mydata,as.numeric))#先要全部改成数值型
  if (fml=='cox'){
    for(i in 1:length(feature)){
      formula <- paste0("Surv","(",tim,",",target,")",'~',feature[i])
      fit_cox<-coxph(as.formula(formula), data = mydata,ties = "breslow")
      result[i,c(1:5)]<-summary(fit_cox)$coefficients[,c(1,3,4,5,2)]
    }

    #predictor <- rownames(result)
    orlow <- exp(result[,1]-1.96*result[,2])
    orup <- exp(result[,1]+1.96*result[,2])
    dtable <- cbind(orlow,orup)
    coefs <- cbind(result,dtable)
    coefs <- round(coefs,round)
    rownames(coefs)  <- NULL
    coefs <- data.frame(name = feature, coefs,row.names = feature)
    colnames(coefs) <- c("Predictor","Estimate","SE","Z","p","Hazard Ratio","Lower","Upper")
    results$cox_coefs <- coefs
    fea<-rownames(coefs[c(which(coefs$p<P)),])##选取显著因子
    if (length(fea)<2){
        x<-data.matrix(mydata[,feature])
        descrip<-paste0(descrip,"在进行Lasso-Cox 特征筛选时将用全部特征进行lasso特征筛选！@")
    }
    else{
        x<-data.matrix(mydata[,fea])
    }

    y<-data.matrix(Surv(as.double(mydata[,tim]),as.double(mydata[,target])))
  }else{
    y<-data.matrix(mydata[,target])#本来是as.matrix
    x<-data.matrix(mydata[,feature])#需要先改成数值型再改成matrix否则会将factor变成字符型
  }
  f1 = glmnet(x, y, family=fml, nlambda=100, alpha=1)
  cvfit=cv.glmnet(x,y, family=fml)#为了避免出错改成data.matrix
  tab<-data.frame(round(data.matrix(coef(cvfit)),round))
  colnames(tab)<-'coef'
  tab['name']<-row.names(tab)
  results$tab<-tab[c('name','coef')]
  #图1
  p1name <- paste0("p1pic",mytime,"_",rand,".png")
  p1name1 <- paste0("p1pic",mytime,"_",rand,".",picFormat)
  png(file=paste0(savePath,p1name),width=7,height=8,units="in",res=dpi)
  plot(f1, xvar="lambda", label=T)#出图1
  abline(v = log(cvfit$lambda.min), lty = 3)
  abline(v = log(cvfit$lambda.1se), lty = 3)
  dev.off()
  if ((picFormat=='svg')|(picFormat=='pdf')|(picFormat=='eps'))
  {
    if (picFormat=='eps'){picFormat='cairo_ps'}
    do.call(picFormat,list(file=paste0(savePath,p1name1),width=7,height=8))
    if (picFormat=='cairo_ps'){picFormat='eps'}
  }
  else
  {
    do.call(picFormat,list(file=paste0(savePath,p1name1),width=7,height=8,units="in",res=dpi))
  }
  plot(f1, xvar="lambda", label=T)#出图1
  abline(v = log(cvfit$lambda.min), lty = 3)
  abline(v = log(cvfit$lambda.1se), lty = 3)
  dev.off()

  results$p1 <- c(p1name,p1name1)
  #图2
  p2name <- paste0("p2pic",mytime,"_",rand,".png")
  p2name1 <- paste0("p2pic",mytime,"_",rand,".",picFormat)
  png(file=paste0(savePath,p2name),width=7,height=8,units="in",res=dpi)
  p2<-plot(cvfit)#出图2
  dev.off()
  if ((picFormat=='svg')|(picFormat=='pdf')|(picFormat=='eps'))
  {
    if (picFormat=='eps'){picFormat='cairo_ps'}
    do.call(picFormat,list(file=paste0(savePath,p2name1),width=7,height=8))
    if (picFormat=='cairo_ps'){picFormat='eps'}
  }
  else
  {
    do.call(picFormat,list(file=paste0(savePath,p2name1),width=7,height=8,units="in",res=dpi))
  }
  p2<-plot(cvfit)#出图2
  dev.off()
  results$p2 <- c(p2name,p2name1)
  #描述
  s1=cvfit$lambda.min#求出最小值
  s2=cvfit$lambda.1se#求出最小值一个标准误的λ值
  mod1<-coef(cvfit$glmnet.fit,s=s1,exact = F)#这两个模型就是最终筛选出的模型
  mod2<-coef(cvfit$glmnet.fit,s=s2,exact = F)#

  m1<-rownames(data.frame(which(mod1[-1,]!=0)))
  m2<-rownames(data.frame(which(mod2[-1,]!=0)))
  descrip<-paste0(descrip,'最小均方误差的λ为',round(s1,round),',对应模型的变量选择为:',paste(m1,collapse = '+'),'@',
                  '最小距离的标准误差的λ为',round(s2,round),',对应模型的变量选择为:',paste(m2,collapse = '+'))
  results$descrip<-enc2utf8(descrip)
  return(results)
}
           ''')

    if len(feature) < 2:
        return {'error': 'LASSO回归中特征组合最少需要两个，请继续添加！' + 'false-error'}
    dv_unique = pd.unique(df_input[dependent_variable])
    if method == 'binomial' and len(dv_unique) > 2:
        return {'error': 'LASSO回归中binomial方法因变量只能是二分类，请重新操作！' + 'false-error'}
    if method == 'cox':
        r_df_input = df_input[[dependent_variable] + [tim] + feature].dropna()
        r_df_input = r_df_input[r_df_input[tim] > 0]  ##cox 中删除时间小于等于0的数据
    else:
        r_df_input = df_input[[dependent_variable] + feature].dropna()

    _, cc, _ = _feature_classification(r_df_input[feature])
    if len(cc) > 0:
        return {'error': 'LASSO回归存在非数值型的分类数据，请将其转化为数值型的数据之后再进行分析！' + 'false-error'}

    with localconverter(ro.default_converter + ro.pandas2ri.converter):
        r_df = ro.conversion.py2rpy(r_df_input)
    target = dependent_variable
    feature = ro.StrVector(feature)

    result = LASSO(mydata=r_df, target=target, feature=feature, tim=tim,
                   fml=method, savePath=savePath, P=0.1, round=decimal_num, dpi=dpi, picFormat=picFormat)

    list_plot_path = []
    list_plot_dict = {}
    table_dic = {}
    list_plot_dict_save = {}
    with localconverter(ro.default_converter + ro.pandas2ri.converter):
        list_plot_path = list(result.rx2('p1'))
        list_plot_dict.update({'图1': list_plot_path[0]})
        list_plot_dict_save.update({'图1': list_plot_path[1]})
        list_plot_path = list(result.rx2('p2'))
        list_plot_dict.update({'图2': list_plot_path[0]})
        list_plot_dict_save.update({'图2': list_plot_path[1]})
        if method == 'cox':
            df_result1 = ro.conversion.rpy2py(result.rx2('cox_coefs'))  # 表格
            table_dic.update({'cox回归系数表': df_result1})
        df_result = ro.conversion.rpy2py(result.rx2('tab'))  # 表格
        table_dic.update({'系数表': df_result})
        str_result = tuple(result.rx2('descrip'))[0]  # 描述
        str_result = str_result.replace('@', '\n')

    result_dict = {'str_result': {'分析结果描述': str_result}, 'tables': table_dic, 'pics': list_plot_dict,
                   'save_pics': list_plot_dict_save}
    return result_dict


7.相关性分析
import datetime
import pandas as pd
import matplotlib
import os

#解耦导入
from AnalysisFunction.celerytest.utils.analysis_dict import _analysis_dict
from AnalysisFunction.celerytest.utils.get_environment import _get_environment_info
from AnalysisFunction.celerytest.utils.data_standardization import data_standardization
from AnalysisFunction.celerytest.utils.fea_importance_models import _logistic2_cfeatures_importance, \
    _lasso_rfeatures_importance, _ridge_rfeatures_importance, _xgboost_rfeatures_importance, \
    _randomforest_rfeatures_importance, _adaboost_rfeatures_importance, _linear_rfeatures_importance, \
    _kneighb_rfeatures_importance, _LinearSVM_rfeatures_importance, _logisticL1_cfeatures_importance, \
    _xgboost_cfeatures_importance, _randomforest_cfeatures_importance, _adaboost_cfeatures_importance, \
    _gussnb_cfeatures_importance, _mlp_cfeatures_importance, _svm_cfeatures_importance, _kneighb_cfeatures_importance, \
    _cnb_cfeatures_importance, _lightgbm_cfeatures_importance, _DecisionTree_cfeatures_importance, \
    _GBDT_cfeatures_importance
from AnalysisFunction.celerytest.utils.round_dec import round_dec

current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
matplotlib.use("AGG")
import matplotlib.pyplot as plt
from sklearn.experimental import enable_halving_search_cv  # noqa
import matplotlib as mpl
import traceback
mpl.rcParams['axes.grid'] = False
plt.rcParams["font.sans-serif"] = ['Times New Roman + SimSun']  # 用来正常显示中文标签
plt.rcParams["axes.unicode_minus"] = False  # 用来正常显示负号
plt.rcParams["ps.useafm"] = True
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams["pdf.fonttype"] = 42

random_model = ['LGBMClassifier', 'XGBClassifier', 'XGBRegressor', 'RandomForestClassifier', 'AdaBoostClassifier','DecisionTreeClassifier'
              'MLPClassifier', 'SVC', 'LogisticRegression', 'LogisticRegressionCV', 'RandomForestRegressor', 'GradientBoostingClassifier'
              'AdaBoostRegressor', 'LinearSVR', 'LassoCV', 'PCA', 'GaussianMixture', 'KMeans', 'SpectralClustering']

env_info = _get_environment_info(__file__)#环境导入

def features_importance(df_input, dependent_variable, independent_variable, top_features=10, model_type=1, standardization=True, output_dir=None, searching=True, dpi=600, image_format="jpeg", decimal_places=3, ):

    """
    重要度排序
    Input:
        df_input:DataFrame 输入的待处理数据
        dependent_variable:str 因变量
        independent_variable:list 自变量
        top_features:图表中展示的特征数量
        model_type:int 使用的模型编号
                    '_lasso_features_importance':LassoCV(),
                    '_ridge_features_importance':RidgeV(),
                    'xgboost_features_importance':XGBClassifier(),
        standardization:bool (2020/12/31) 是否先对数据进行标准化处理，默认为True,
        searching:bool 是否进行自动寻参，默认为True
        output_dir:str 图片存储路径
        image_format：图片保存格式
        decimal_places：结果保留位数
    """
    try:

        df_temp = df_input[independent_variable + [dependent_variable]].dropna().reset_index().drop(columns="index")
        if model_type != 15:
            value_max = 2000000
        else:
            value_max = 200000
        if df_temp.shape[0] * df_temp.shape[1] > value_max:
            return {
                "error": "数据量过大：样本数为"
                + str(df_temp.shape[0])
                + "，因子数："
                + str(df_temp.shape[1])
                + "。请减少样本量或者减少因子数量！(目前数据暂时只支持分析数据矩阵最大为"
                + str(value_max)
                +")"
            }

        if standardization:
            if model_type==17:
                mmethod = "MinMaxScaler"
            else:
                mmethod='StandardScaler'
            rresult_dict = data_standardization(df_temp, independent_variable, method=mmethod)
            _, df_temp, _, _, _, _ = _analysis_dict(rresult_dict)

        if model_type > 8 and len(pd.unique(df_temp[dependent_variable])) > 8:
            return {"error": "暂不允许类别数目大于5的情况。请检查因变量取值情况。"+"false-error"}

        model_name_map = {
            # 回归
            1: "Lasso Regression",
            2: "Ridge Regression",
            3: "XGBoost Regressor",
            4: "Random Forest Regressor",
            5: "AdaBoost Regressor",
            6: "Linear Regression",
            7: "K-Neighbors Regressor",
            8: "Linear SVM Regressor",
            # 分类
            9: "Logistic Regression (L1)",
            10: "XGBoost Classifier",
            11: "Random Forest Classifier",
            12: "AdaBoost Classifier",
            13: "Gaussian Naive Bayes",
            14: "MLP Classifier",
            15: "SVM Classifier",
            16: "K-Neighbors Classifier",
            17: "Complement Naive Bayes",
            18: "LightGBM Classifier",
            19: "Decision Tree Classifier",
            20: "Gradient Boosting Classifier",
            21: "Logistic Regression",
        }

        #模型字典
        model_dispatch = {
            # 回归
            1: (_lasso_rfeatures_importance, False),
            2: (_ridge_rfeatures_importance, False),
            3: (_xgboost_rfeatures_importance, True),
            4: (_randomforest_rfeatures_importance, True),
            5: (_adaboost_rfeatures_importance, True),
            6: (_linear_rfeatures_importance, True),
            7: (_kneighb_rfeatures_importance, True),
            8: (_LinearSVM_rfeatures_importance, True),
            # 分类
            9: (_logisticL1_cfeatures_importance, False),
            10: (_xgboost_cfeatures_importance, True),
            11: (_randomforest_cfeatures_importance, True),
            12: (_adaboost_cfeatures_importance, True),
            13: (_gussnb_cfeatures_importance, True),
            14: (_mlp_cfeatures_importance, True),
            15: (_svm_cfeatures_importance, True),
            16: (_kneighb_cfeatures_importance, True),
            17: (_cnb_cfeatures_importance, True),
            18: (_lightgbm_cfeatures_importance, True),
            19: (_DecisionTree_cfeatures_importance, True),
            20: (_GBDT_cfeatures_importance, True),
            21: (_logistic2_cfeatures_importance, False),
        }
        # plt.close()

        # 模型查找与执行
        model_config = model_dispatch.get(model_type)

        if not model_config:
            return {"error": f"指定的 model_type 无效: {model_type}"}

        model_function, requires_searching = model_config
        #映射赋值
        x_columns = independent_variable
        y_column = dependent_variable

        # 准备通用的函数参数
        call_args = {
            "df_input": df_temp,
            "x_columns": x_columns,
            "y_column": y_column,
            "top_features": top_features,
            "output_dir": output_dir,
            "dpi": dpi,
            "image_format": image_format,
        }
        #如果需要超参数搜索，添加searching
        if requires_searching:
            call_args["searching"] = searching

        # 使用字典解包 `**` 来调用函数，传入所有参数
        df_result, str_result, plot_name_dict, plot_name_dict_save = model_function(**call_args)
        plt.close()

        # 整合描述信息
        description_text = str_result  # str_result 是从模型函数返回的
        if model_type <= 5:
            description_text += "接下来可使用这些相关度较高的变量进行，利用左侧栏‘机器学习回归’进一步的细致化回归建模。"
        else:
            description_text += "接下来可使用这些相关度较高的变量进行，利用左侧栏‘机器学习分类’进一步的细致化分类建模。"

        model_name = model_name_map.get(model_type, f"未知模型 {model_type}") #模型名映射

        # 监控图片生成
        figures_list = []
        for i, (key, saved_file_name) in enumerate(plot_name_dict_save.items()):
            try:
                # 检查文件
                if os.path.exists(os.path.join(output_dir, saved_file_name)):
                    # 从文件名推断格式
                    base_name, ext = os.path.splitext(saved_file_name)
                    current_format = ext.strip('.')

                    figure_info = {
                        "name": f"重要度排序图_{i + 1}",
                        "file": {"png": f"{base_name}.png", current_format: saved_file_name},
                        "description": f"模型 {model_name} 生成的特征重要性排序图。"
                    }
                    figures_list.append(figure_info)
                else:
                    raise FileNotFoundError("模型函数报告已保存，但文件未找到。")
            except Exception as e:
                # 如果文件保存失败或路径处理出错，则记录失败信息
                failure_figure_info = {
                    "name": f"重要度排序图_{i + 1}",
                    "file": {},
                    "description": f"这个图生成失败: {str(e)}"
                }
                figures_list.append(failure_figure_info)

        tables_list = []
        df_result = df_result.applymap(lambda x: round_dec(x, Decimal_places=decimal_places))
        tables_list.append({
            "name": "重要度排序",
            "file": {"dataframe": df_result},
            "description": "包含各个特征的重要性得分及其排序的详细数据表。"
        })

        # 构建最终的返回字典
        result_dict = {
            "status": "success",
            "description": description_text,
            "tables": tables_list,
            "figures": figures_list,
            "environment": env_info
        }
        return result_dict
# 将这部分代码添加到 features_importance 函数的末尾，作为 try 块的补充
    except ValueError as e:
        # 捕获可预见的、主动抛出的错误
        return {
            "status": "error",
            "description": str(e),
            "tables": [],
            "figures": [],
            "environment": env_info
        }

    except Exception as e:
        # 捕获所有其他意外的运行时错误
        error_message = traceback.format_exc()
        print(error_message) # 在后台打印详细追溯信息，便于调试
        return {
            "status": "exception",
            "description": f"发生意外错误: {str(e)}\n\nTraceback:\n{error_message}",
            "tables": [],
            "figures": [],
            "environment": env_info
        }

# 使用格式化的时间字符串作为文件夹name
output_dir = fr'E:\xsmart\AI_0909\static\exel\{current_time}/' # 保存路径
os.mkdir(output_dir)



"""
    辅助工具函数，用于进行对应算法的特征重要性计算
"""

import pandas as pd
import numpy as np
import matplotlib

#解耦导入
from AnalysisFunction.celerytest.utils.Hyperparameters import GridDefaultRange
from AnalysisFunction.celerytest.utils.filter_dict_to_str import dictionary_string
from AnalysisFunction.celerytest.utils.horizontal_bar_plot import horizontal_bar_plot

matplotlib.use("AGG")
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder
from sklearn.inspection import permutation_importance
from xgboost import XGBRegressor
from sklearn.linear_model import LassoCV, RidgeCV
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.svm import LinearSVR
from sklearn.neighbors import KNeighborsRegressor
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import ComplementNB
from sklearn.svm import SVC
from sklearn.experimental import enable_halving_search_cv  # noqa
from sklearn.model_selection import RandomizedSearchCV
import matplotlib as mpl

mpl.rcParams['axes.grid'] = False
plt.rcParams["font.sans-serif"] = ['Times New Roman + SimSun']  # 用来正常显示中文标签
plt.rcParams["axes.unicode_minus"] = False  # 用来正常显示负号)
plt.rcParams["ps.useafm"] = True
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams["pdf.fonttype"] = 42

random_model = ['LGBMClassifier', 'XGBClassifier', 'XGBRegressor', 'RandomForestClassifier', 'AdaBoostClassifier','DecisionTreeClassifier'
              'MLPClassifier', 'SVC', 'LogisticRegression', 'LogisticRegressionCV', 'RandomForestRegressor', 'GradientBoostingClassifier'
              'AdaBoostRegressor', 'LinearSVR', 'LassoCV', 'PCA', 'GaussianMixture', 'KMeans', 'SpectralClustering']


def _lasso_rfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]
    alpharange = np.logspace(-10, -2, 200, base=10)

    lasso_ = LassoCV(alphas=alpharange, cv=5,random_state=42).fit(x, y)
    param_dict = lasso_.get_params()
    param_dict["alpha"] = lasso_.alpha_
    str_result = "算法：lasso回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用lasso Regressor进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, lasso_.__class__.__name__
    )

    c1 = {"Variable": x_columns, "Weight Importance": abs(lasso_.coef_)}
    a1 = pd.DataFrame(c1)
    df_result = a1.sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )
    str_result += "\n注意：在使用Lasso进行重要度排序时，由于其线性特性，将默认所有变量具有相同量纲和值域。这一假设在勾选数据标准化后可认为成立。如果线性模型(与常识对比)在此数据集上表现较差，可考虑使用非线性模型如XGBoost。"

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _ridge_rfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]
    alpharange = np.logspace(-10, -2, 200, base=10)

    Ridge_ = RidgeCV(alphas=alpharange, cv=5).fit(x, y)
    param_dict = Ridge_.get_params()
    param_dict["alpha"] = Ridge_.alpha_
    str_result = "算法：岭回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用Ridge Regressor进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, Ridge_.__class__.__name__
    )

    c1 = {"Variable": x_columns, "Weight Importance": abs(Ridge_.coef_)}
    a1 = pd.DataFrame(c1)
    df_result = a1.sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )
    str_result += "\n注意：在使用岭回归进行重要度排序时，由于其线性特性，将默认所有变量具有相同量纲和值域。这一假设在勾选数据标准化后可认为成立。如果线性模型(与常识对比)在此数据集上表现较差，可考虑使用非线性模型如XGBoost。"

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _xgboost_rfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    model: XGBOOST模型，如果不传则自动产生一个自动寻参后的XGBOOST模型
    searching: 是否自动寻参，默认为是
    output_dir:str 图片存储路径

    hyperparams: XGBClassifier params -- no selection yet
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("Regression", XGBRegressor())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(XGBRegressor(), param_distributions=GridDefaultRange['XGBRegressor'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = XGBRegressor(**model.best_params_)
    else:
        model = XGBRegressor(random_state=42)
    # if searching:
    #     str_result = "采用XGBoost进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'XGBRegressor')
    # else:
    str_result = "算法：XGBoost回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用XGBoost进行变量重要度分析，模型参数为:\n" + dictionary_string(model.get_params(), model.__class__.__name__)
    model.fit(x, y)

    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(model.feature_importances_)}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]



def _randomforest_rfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("Regression", RandomForestRegressor())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(RandomForestRegressor(), param_distributions=GridDefaultRange['RandomForestRegressor'],random_state=42, )
        #searcher = BayesSearchCV(RandomForestRegressor(),  search_spaces=BayesDefaultRange['RandomForestRegressor']  # 使用字符串键)
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = RandomForestRegressor(**model.best_params_).fit(x, y)
    else:
        model = RandomForestRegressor(random_state=42).fit(x, y)
    param_dict = model.get_params()

    # if searching:
    #     str_result = "采用Random Forrest Regressor进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'RandomForestRegressor')
    # else:
    str_result = "算法：随机森林回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用Random Forrest Regressor进行变量重要度分析，模型参数为:\n" + dictionary_string(
            param_dict, model.__class__.__name__)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(model.feature_importances_)}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _adaboost_rfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("Regression", AdaBoostRegressor())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(AdaBoostRegressor(), param_distributions=GridDefaultRange['AdaBoostRegressor'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = AdaBoostRegressor(**model.best_params_).fit(x, y)
    else:
        model = AdaBoostRegressor(random_state=42).fit(x, y)
    param_dict = model.get_params()

    # if searching:
    #     str_result = "采用AdaBoost Regressor进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'AdaBoostRegressor')
    # else:
    str_result = "算法：AdaBoost回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用AdaBoost Regressor进行变量重要度分析，模型参数为:\n" + dictionary_string(
            param_dict, model.__class__.__name__)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(model.feature_importances_)}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _linear_rfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    model = LinearRegression(fit_intercept=True)
    model.fit(x, y)
    #获取模型参数
    param_dict = model.get_params()
    #将模型参数转化为字符串
    str_result = "算法：线性回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用Linear Regression进行变量重要度分析，模型参数为:\n" + dictionary_string(
            param_dict, model.__class__.__name__)
    #获取系数并排序
    df_result = pd.DataFrame({
        "Variable": x_columns,
        "Weight Importance": abs(model.coef_)
    }).sort_values(by="Weight Importance", ascending=False)
    #获取重要度最高的特征
    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )
    str_result += "\n注意：在使用线性回归进行重要度排序时，由于其线性特性，将默认所有变量具有相同量纲和值域。这一假设在勾选数据标准化后可认为成立。如果线性模型(与常识对比)在此数据集上表现较差，可考虑使用非线性模型如XGBoost。"

    plot_name_dict = {}
    #绘制图形
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]

def _kneighb_rfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna().sample(frac=1, random_state=42).reset_index(drop=True)
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", KNeighborsRegressor())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(KNeighborsRegressor(),
                                      param_distributions=GridDefaultRange['KNeighborsRegressor'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = KNeighborsRegressor(**model.best_params_).fit(x, y)
    else:
        model = KNeighborsRegressor().fit(x, y)
    #获取模型参数
    param_dict = model.get_params()
    #将模型参数转化为字符串
    str_result = "算法：K近邻回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用K近邻回归进行变量重要度分析，模型参数为:\n" + dictionary_string(
            param_dict, model.__class__.__name__)
    #获取系数并排序
    weight_im = abs(
        permutation_importance(
            model, x, y, n_repeats=10, random_state=0
        ).importances_mean
    )
    weight_im = weight_im / sum(weight_im)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": weight_im}
    ).sort_values(by="Weight Importance", ascending=False)

    #获取重要度最高的特征
    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    #绘制图形
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]

def _LinearSVM_rfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", LinearSVR())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(LinearSVR(), param_distributions=GridDefaultRange['LinearSVR'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = LinearSVR(**model.best_params_).fit(x, y)
    else:
        model = LinearSVR(random_state=42).fit(x, y)
    param_dict = model.get_params()
    #将模型参数转化为字符串
    str_result = "算法：线性支持向量机回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用线性支持向量机分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, model.__class__.__name__)
    # if searching:
    #     str_result = "采用支持向量机分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'LinearSVR')
    # else:
    #     str_result = "采用支持向量机分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
    #         param_dict, model.__class__.__name__)
    #排序
    weight_im = abs(
        permutation_importance(
            model, x, y, n_repeats=10, random_state=0
        ).importances_mean
    )
    weight_im = weight_im / sum(weight_im)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": weight_im}
    ).sort_values(by="Weight Importance", ascending=False)
    #获取重要度最高的特征
    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )
    #画图
    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]



# ----------分类重要度排序-----------
def _logisticL1_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]
    crange = np.logspace(-9, 1, 200, base=10)

    logicv_ = LogisticRegressionCV(Cs=crange, cv=5, penalty="l1", solver="saga",random_state=42).fit(
        x, y
    )
    param_dict = logicv_.get_params()
    param_dict["C"] = logicv_.C_
    str_result = "算法：逻辑回归分类模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用L1正则化的Logistic回归进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, logicv_.__class__.__name__
    )

    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(logicv_.coef_[0])}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )
    str_result += "\n注意：在使用Logistiv+L1进行重要度排序时，由于其指数部分的线性形式，将默认所有变量具有相同量纲和值域。这一假设在勾选数据标准化后可认为成立。如果线性模型(与常识对比)在此数据集上表现较差，可考虑使用非线性模型如XGBoost。"

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]

def _logistic2_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]
    # 对因变量进行有序编码
    encoder = OrdinalEncoder()
    y_encoded = encoder.fit_transform(y.values.reshape(-1, 1)).ravel()

    logicv = LogisticRegression(penalty='l1', solver='saga', multi_class='multinomial', random_state=42).fit(
        x, y_encoded
    )
    param_dict = logicv.get_params()
    param_dict["C"] = logicv.C
    str_result = "算法：有序逻辑回归模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用有序逻辑回归进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, logicv.__class__.__name__
    )

    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(logicv.coef_[0])}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )
    str_result += "\n注意：函数对结局变量进行了有序编码。这是有序逻辑回归所必需的。用户需要确保结局变量是有序的分类变量。"
    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _xgboost_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    model: XGBOOST模型，如果不传则自动产生一个自动寻参后的XGBOOST模型
    searching: 是否自动寻参，默认为是
    output_dir:str 图片存储路径

    hyperparams: XGBClassifier params -- no selection yet
    """
    x = df_input[x_columns]
    y = df_input[y_column]

    if searching:
        # searcher = RandSearcherCV("Classification", XGBClassifier())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(XGBClassifier(), param_distributions=GridDefaultRange['XGBClassifier'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = XGBClassifier(**model.best_params_).fit(x, y)
    else:
        model = XGBClassifier(random_state=42).fit(x, y)
    # if searching:
    #     str_result = "采用极端梯度提升树(XGBOOST)进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'XGBClassifier')
    # else:
    str_result = "算法：XGBoost分类模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用极端梯度提升树(XGBOOST)进行变量重要度分析，模型参数为:\n" + dictionary_string(
            model.get_params(), model.__class__.__name__)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(model.feature_importances_)}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )
    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _randomforest_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("Classification", RandomForestClassifier())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(RandomForestClassifier(),param_distributions=GridDefaultRange['RandomForestClassifier'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = RandomForestClassifier(**model.best_params_).fit(x, y)
    else:
        model = RandomForestClassifier(random_state=42).fit(x, y)
    param_dict = model.get_params()


    # if searching:
    #     str_result = "采用Random Forrest Classifier进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'RandomForestClassifier')
    # else:
    str_result = "算法：随机森林模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用Random Forrest Classifier进行变量重要度分析，模型参数为:\n" + dictionary_string(
            param_dict, model.__class__.__name__)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(model.feature_importances_)}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _adaboost_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", AdaBoostClassifier())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(AdaBoostClassifier(), param_distributions=GridDefaultRange['AdaBoostClassifier'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = AdaBoostClassifier(**model.best_params_).fit(x, y)
    else:
        model = AdaBoostClassifier(random_state=42).fit(x, y)
    param_dict = model.get_params()

    str_result = "算法：AdaBoost分类模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用AdaBoost Classifier进行变量重要度分析，模型参数为:\n" + dictionary_string(
                param_dict, model.__class__.__name__)


    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(model.feature_importances_)}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]

def _DecisionTree_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:

        searcher = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=GridDefaultRange['DecisionTreeClassifier'],random_state=42, )
        model = searcher.fit(x, y)
        model = DecisionTreeClassifier(**model.best_params_).fit(x, y)
    else:
        model = DecisionTreeClassifier(random_state=42).fit(x, y)
    param_dict = model.get_params()
    str_result = "算法：决策树分类模型\n"
    str_result +="变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用DecisionTree进行变量重要度分析，模型参数为:\n" + dictionary_string(
                param_dict, model.__class__.__name__)


    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(model.feature_importances_)}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]

def _GBDT_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        searcher = RandomizedSearchCV(GradientBoostingClassifier(), param_distributions=GridDefaultRange['GradientBoostingClassifier'],random_state=42, )
        model = searcher.fit(x, y)
        model = GradientBoostingClassifier(**model.best_params_).fit(x, y)
    else:
        model = GradientBoostingClassifier(random_state=42).fit(x, y)
    param_dict = model.get_params()
    str_result = "算法：GBDT分类模型\n"
    str_result += "变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用GBDT进行变量重要度分析，模型参数为:\n" + dictionary_string(
                param_dict, model.__class__.__name__)


    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": abs(model.feature_importances_)}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])

    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]

def _gussnb_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", GaussianNB())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(GaussianNB(), param_distributions=GridDefaultRange['GaussianNB'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = GaussianNB(**model.best_params_).fit(x, y)
    else:
        model = GaussianNB().fit(x, y)
    param_dict = model.get_params()


    # if searching:
    #     str_result = "采用高斯朴素贝叶斯分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'GaussianNB')
    # else:
    str_result = "算法：高斯朴素贝叶斯分类模型\n"
    str_result += "变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用高斯朴素贝叶斯分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
            param_dict, model.__class__.__name__)
    weight_im = abs(
        permutation_importance(
            model, x, y, n_repeats=10, random_state=0
        ).importances_mean
    )
    weight_im = weight_im / sum(weight_im)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": weight_im}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _cnb_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", ComplementNB())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(ComplementNB(), param_distributions=GridDefaultRange['ComplementNB'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = ComplementNB(**model.best_params_).fit(x, y)
    else:
        model = ComplementNB().fit(x, y)
    param_dict = model.get_params()
    str_result = "算法：补朴素贝叶斯分类模型\n"
    str_result += "变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用补朴素贝叶斯分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, model.__class__.__name__)
    # if searching:
    #     str_result = "采用补朴素贝叶斯分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'ComplementNB')
    # else:
    #     str_result = "采用补朴素贝叶斯分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
    #         param_dict, model.__class__.__name__)
    weight_im = abs(
        permutation_importance(
            model, x, y, n_repeats=10, random_state=0
        ).importances_mean
    )
    weight_im = weight_im / sum(weight_im)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": weight_im}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _mlp_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", MLPClassifier())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(MLPClassifier(), param_distributions=GridDefaultRange['MLPClassifier'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = MLPClassifier(**model.best_params_).fit(x, y)
    else:
        model = MLPClassifier(random_state=42).fit(x, y)
    param_dict = model.get_params()
    str_result = "算法：神经网络分类模型\n"
    str_result += "变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用多层感知器（神经网络）分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, model.__class__.__name__)
    # if searching:
    #     str_result = "采用多层感知器（神经网络）分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'XGBRegressor')
    # else:
    #     str_result = "采用多层感知器（神经网络）分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
    #         param_dict, model.__class__.__name__)
    weight_im = abs(
        permutation_importance(
            model, x, y, n_repeats=10, random_state=0
        ).importances_mean
    )
    weight_im = weight_im / sum(weight_im)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": weight_im}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _svm_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", SVC())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(SVC(), param_distributions=GridDefaultRange['SVC'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = SVC(**model.best_params_).fit(x, y)
    else:
        model = SVC(random_state=42).fit(x, y)
    param_dict = model.get_params()
    str_result = "算法：支持向量机分类模型\n"
    str_result += "变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用支持向量机分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, model.__class__.__name__)
    # if searching:
    #     str_result = "采用支持向量机分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'SVC')
    # else:
    #     str_result = "采用支持向量机分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
    #         param_dict, model.__class__.__name__)
    weight_im = abs(
        permutation_importance(
            model, x, y, n_repeats=10, random_state=0
        ).importances_mean
    )
    weight_im = weight_im / sum(weight_im)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": weight_im}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _kneighb_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", KNeighborsClassifier())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=GridDefaultRange['KNeighborsClassifier'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = KNeighborsClassifier(**model.best_params_).fit(x, y)
    else:
        model = KNeighborsClassifier().fit(x, y)
    param_dict = model.get_params()

    # if searching:
    #     str_result = "采用K近邻分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'KNeighborsClassifier')
    # else:
    str_result = "算法：K近邻分类模型\n"
    str_result += "变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用K近邻分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
            param_dict, model.__class__.__name__)
    weight_im = abs(
        permutation_importance(
            model, x, y, n_repeats=10, random_state=0
        ).importances_mean
    )
    weight_im = weight_im / sum(weight_im)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": weight_im}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


def _lightgbm_cfeatures_importance(
    df_input,
    x_columns,
    y_column,
    top_features,
    searching=True,
    output_dir=None,
    dpi=600,
    image_format="jpeg",
):
    """
    df_input:Dataframe
    x_columns:自变量list
    y_column：因变量str
    top_features:图表中展示的特征数量
    (会剔除空值)
    output_dir:str 图片存储路径

    hyperparams: alpha(alpharange), cv, tol
    """
    dftemp = df_input[x_columns + [y_column]].dropna()
    x = dftemp[x_columns]
    y = dftemp[y_column]

    if searching:
        # searcher = RandSearcherCV("classification", LGBMClassifier())
        # model = searcher(x, y)  # ; searcher.report()
        searcher = RandomizedSearchCV(LGBMClassifier(), param_distributions=GridDefaultRange['LGBMClassifier'],random_state=42, )
        # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
        model = searcher.fit(x, y)
        model = LGBMClassifier(**model.best_params_).fit(x, y)
    else:
        model = LGBMClassifier(random_state=42).fit(x, y)
    param_dict = model.get_params()
    str_result = "算法：LinghtGBM分类模型\n"
    str_result += "变量：结局变量：{}，特征变量：{}\n".format(y_column, ', '.join(x_columns))
    str_result += "采用LightGBM分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
        param_dict, model.__class__.__name__)
    # if searching:
    #     str_result = "采用LightGBM分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(model.best_params_, 'LGBMClassifier')
    # else:
    #     str_result = "采用LightGBM分类算法进行变量重要度分析，模型参数为:\n" + dictionary_string(
    #         param_dict, model.__class__.__name__)
    weight_im = abs(
        permutation_importance(
            model, x, y, n_repeats=10, random_state=0
        ).importances_mean
    )
    weight_im = weight_im / sum(weight_im)
    df_result = pd.DataFrame(
        {"Variable": x_columns, "Weight Importance": weight_im}
    ).sort_values(by="Weight Importance", ascending=False)

    top_list = list(df_result.head(top_features)["Variable"])
    str_result += "\n重要度最高的前{0}个变量（由高到低）分别为：{1}。\n".format(
        len(top_list), str(top_list)[1:-1]
    )

    plot_name_dict = {}
    if output_dir is not None:
        plot_name_dict = horizontal_bar_plot(
            df_result.head(top_features).sort_values(
                by="Weight Importance", ascending=True
            ),
            "Variable",
            "Weight Importance",
            "Feature Importance (Coefficient)",
            output_dir,
            dpi=dpi,
            image_format=image_format,
        )
    return df_result, str_result, plot_name_dict["pics"], plot_name_dict["save_pics"]


（7.2-7.8共用上面的）

8.xgboost部分
'''
@function：机器学习二分类
@Author  ：zhu
@Date    ：2025/11/24 09:23
'''
import os
import random
import datetime
import pickle
import math
import time
import pandas as pd
import numpy as np
import matplotlib
import joblib
from skopt import BayesSearchCV
from statsmodels.formula.api import ols
current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
import os
from joblib.externals.loky.backend.resource_tracker import ResourceTracker
from AnalysisFunction.utils_ml.GeneticSearch_ML import GeneticSearchCV
from AnalysisFunction.utils_ml.Hyper import HyperoptSearchCV
from AnalysisFunction.utils_ml.params import BayesDefaultRange, GeneticDefaultRange, HyperDefaultRange

matplotlib.use("AGG")
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.base import clone
from sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler
from sklearn.inspection import permutation_importance
from skopt import BayesSearchCV
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split as TTS
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import RandomizedSearchCV
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK
from xgboost import XGBRegressor
from sklearn.linear_model import LassoCV, RidgeCV
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.svm import LinearSVR
from sklearn.neighbors import KNeighborsRegressor

from sklearn.mixture import GaussianMixture
from sklearn.cluster import Birch
from sklearn.cluster import KMeans
from sklearn.cluster import AffinityPropagation
from sklearn.cluster import SpectralClustering
from sklearn.cluster import AgglomerativeClustering
#
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import ComplementNB
from sklearn.svm import SVC

from sklearn.feature_selection import RFECV
from sklearn.decomposition import PCA
from AnalysisFunction.utils_ml.FeatrueSelect import mrmr_classif
from AnalysisFunction.utils_ml.FeatrueSelect import ReliefF

from sklearn.metrics import precision_recall_curve, make_scorer
from sklearn.metrics import average_precision_score
from sklearn.metrics import auc
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    mutual_info_score,
    v_measure_score,
    normalized_mutual_info_score,
    silhouette_samples,
)

from sklearn.metrics import brier_score_loss
from sklearn.calibration import CalibratedClassifierCV, calibration_curve

from sklearn.preprocessing import label_binarize

from xgboost import plot_importance
import AnalysisFunction.X_5_SmartPlot as x5
from AnalysisFunction.X_5_SmartPlot import plot_calibration_curve
from AnalysisFunction.X_5_SmartPlot import calculate_net_benefit
from AnalysisFunction.X_5_SmartPlot import plot_decision_curves
from AnalysisFunction.X_1_DataGovernance import data_standardization
from AnalysisFunction.X_1_DataGovernance import _analysis_dict
from AnalysisFunction.X_2_DataSmartStatistics import comprehensive_smart_analysis

from AnalysisFunction.utils_ml import filtering, dic2str, round_dec, save_fig
from AnalysisFunction.utils_ml import (
    classification_metric_evaluate,
    regression_metric_evaluate,
)
from AnalysisFunction.utils_ml import (
    make_class_metrics_dict,
    make_regr_metrics_dict,
    multiclass_metric_evaluate,
)
from AnalysisFunction.utils_ml import ci

# from AnalysisFunction.utils_ml import (
#     GridSearcherCV,
#     RandSearcherCV,
#     GridSearcherSelf,
#     RandSearcherSelf,
# )
from AnalysisFunction.utils_ml.params import GridDefaultRange, BayesDefaultRange
from sklearn.experimental import enable_halving_search_cv  # noqa
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,HalvingGridSearchCV,HalvingRandomSearchCV
from AnalysisFunction.utils_ml.params import RandDefaultRange
from AnalysisFunction.utils_ml.auc_delong import delong_roc_test

from functools import reduce
from yellowbrick.cluster import SilhouetteVisualizer
from yellowbrick.cluster import KElbowVisualizer
from yellowbrick.utils import check_fitted
from yellowbrick.classifier import ConfusionMatrix
import matplotlib as mpl
mpl.rcParams['axes.grid'] = False


plt.rcParams["font.sans-serif"] = ['Times New Roman + SimSun']  # 用来正常显示中文标签

plt.rcParams["axes.unicode_minus"] = False  # 用来正常显示负号)

plt.rcParams["ps.useafm"] = True
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams["pdf.fonttype"] = 42

random_model = ['LGBMClassifier', 'XGBClassifier', 'XGBRegressor', 'RandomForestClassifier', 'AdaBoostClassifier','DecisionTreeClassifier'
              'MLPClassifier', 'SVC', 'LogisticRegression', 'LogisticRegressionCV', 'RandomForestRegressor', 'GradientBoostingClassifier'
              'AdaBoostRegressor', 'LinearSVR', 'LassoCV', 'PCA', 'GaussianMixture', 'KMeans', 'SpectralClustering']


def _send(self, cmd, name, rtype):
    if len(name) > 512:
        # posix guarantees that writes to a pipe of less than PIPE_BUF
        # bytes are atomic, and that PIPE_BUF >= 512
        raise ValueError("name too long")
    msg = f"{cmd}:{name}:{rtype}\n".encode()
    nbytes = os.write(self._fd, msg)
    assert nbytes == len(msg)

ResourceTracker._send = _send


def ML_Classfication(
    df,
    group,
    features,
    decimal_num=3,
    validation_ratio=0.15,
    test_ratio=0.15,
    scoring="roc_auc",
    method="KNeighborsClassifier",
    isKFold="cross",
    n_splits=10,
    explain=True,
    shapSet=2,
    explain_numvar=2,
    explain_sample=2,
    shap_catter=False,
    shap_catter_feas=[],
    searching="default",
    auto_model="RandomizedSearchCV",
    validationCurve=False,
    smooth=False,
    savePath=None,
    style='lancet',
    dpi=600,
    picFormat="jpeg",
    label="LABEL",#标签名
    testSet=False,
    trainSet=False,
    modelSave=True,
    datasave=False,
    testLabel=2,
    trainLabel=0,
    randomState=42,
    resultType=0,
    DCA_cut=0,
    shap_waterfall=True,
    **kwargs,
):
    def _plot_conffusion_matrix(clf, x_data, y_data, un_group, resThreshold, name, dic_name, str_time, savePath=None,
                                picFormat='jpeg', dpi=600):
        plot_name_dict, plot_name_dict_save = {}, {}
        from sklearn.metrics import confusion_matrix
        y_prob = clf.predict_proba(x_data)[:, 1]  # 获取属于正类的概率
        threshold = resThreshold
        # 将概率转换为类别标签
        y_pred_custom = (y_prob > threshold).astype(int)
        con_mx = confusion_matrix(y_data, y_pred_custom, labels=un_group)
        fig, axes = plt.subplots(figsize=(6, 6), dpi=dpi)

        cmap = plt.get_cmap('Blues')
        plt.imshow(con_mx, interpolation='nearest', cmap=cmap)
        plt.colorbar()

        #标签
        indices = range(len(con_mx))
        plt.xticks(indices, un_group)
        plt.yticks(indices, un_group)
        plt.xlabel('预测标签')
        plt.ylabel('真实标签')

        thresh = con_mx.max() / 2.
        for i, j in np.ndindex(con_mx.shape):
            plt.text(j, i, format(con_mx[i, j], 'd'),
                     horizontalalignment='center',
                     color='white' if con_mx[i, j] > thresh else 'black')

        plt.tight_layout()

        plot_name = name + str_time + ".png"
        plot_name_save = name + str_time + "." + picFormat
        plt.savefig(savePath + plot_name, dpi=dpi, format='png', bbox_inches='tight')
        plt.savefig(savePath + plot_name_save, dpi=dpi, format=picFormat, bbox_inches='tight')
        plt.close(fig)
        plot_name_dict.update({dic_name: plot_name})
        plot_name_dict_save.update({dic_name: plot_name_save})
        return plot_name_dict, plot_name_dict_save

    """
    机器学习分类分析

    Input:
        df_input:DataFrame 输入的待处理数据
        group_name:str 分组名
        validation_ratio:float 验证集比例 
        test_ratio:float 测试集比例 
        isKFold:str "cross"交叉验证，nest:嵌套交叉验证 resample：重采样 resample1：无 
        scoring:str 目标评价指标
        method:str 使用的机器学习分类方法/模型
                    'LogisticRegression':LogisticRegression(**kwargs),
                    'XGBClassifier':XGBClassifier(**kwargs),
                    'RandomForestClassifier':RandomForestClassifier(**kwargs),
                    'SVC':SVC(**kwargs),
                    'KNeighborsClassifier':KNeighborsClassifier(**kwargs),
        n_splits:int 交叉验证的子集数目
        explain:bool 是否进行模型解释
        explain_numvar:int 需要解释的变量数
        explain_sample:int 需要例释的样本数
        searching:str 是否进行自动寻参，默认为否 ("default", "handle", "auto")
        auto_model:str 自动寻参的方法 ("RandomizedSearchCV", "GridSearchCV", "BayesSearchCV", "HalvingGridSearchCV", "HalvingRandomSearchCV", "GeneticSearchCV", "Hyperopt")
        validationCurve:bool 是否绘制交叉验证/重采样折叠的 ROC 曲线
        smooth:bool 是否对测试集 ROC 曲线进行平滑处理
        savePath:str 图片存储路径
        style:str 绘图风格 ("lancet", "nejm", "jama", "npg") # 保留自 ML_combine.py
        dpi:int 图片分辨率
        picFormat:str 图片格式 ("jpeg", "png", "pdf", "svg"等)
        label:str 数据集中用于划分测试集/训练集的标签列名 # 标签名
        testSet:bool 是否根据 label 列划分测试集
        trainSet:bool 当 testSet=True 且 isKFold='resample1' 时，是否根据 label 列划分训练集
        modelSave:bool 是否保存训练好的模型
        datasave:bool 是否保存带有预测结果的数据表
        testLabel:int/str 测试集的标签值 # 测试集标签
        trainLabel:int/str 训练集的标签值 (仅用于 testSet=True 且 isKFold='resample1') # 训练集标签
        randomState:int 随机状态种子，用于复现结果
        resultType:int 结果表格中置信区间/标准差的显示类型 (0: SD, 1: CI)
        DCA_cut:float 决策曲线分析图的横坐标截断值
        **kwargs:dict 使用机器学习分类方法的参数

    Return:
        result_dict: dict 包含分析结果的字典
            str_result: dict 分析结果描述
            tables: dict 结果数据表字典
            pics: dict 主要图片文件名字典 (用于显示)
            save_pics: dict 所有保存的图片文件名字典 (用于下载)
            model: dict 保存的模型信息
    """
    palette_dict = {
        'lancet': ["#00468BFF", "#ED0000FF", "#42B540FF", "#0099B4FF", "#925E9FFF", "#FDAF91FF", "#AD002AFF",
                   "#ADB6B6FF","#1B1919FF"],
        'nejm': ["#BC3C29FF", "#0072B5FF", "#E18727FF", "#20854EFF", "#7876B1FF", "#6F99ADFF", "#FFDC91FF", "#EE4C97FF",
                 "#BC3C29FF"],
        'jama': ["#374E55FF", "#DF8F44FF", "#00A1D5FF", "#B24745FF", "#79AF97FF", "#6A6599FF", "#80796BFF", "#374E55FF",
             "#DF8F44FF"],
        'npg': ["#E64B35FF", "#4DBBD5FF", "#00A087FF", "#3C5488FF", "#F39B7FFF", "#8491B4FF", "#91D1C2FF", "#DC0000FF",
                "#7E6148FF", "#B09C85FF"]}

    name_dict = { # 模型名称字典
        "LogisticRegression": "logistic",
        "XGBClassifier": "XGBoost",
        "RandomForestClassifier": "RandomForest",
        "LGBMClassifier": "LightGBM",
        "SVC": "SVM",
        "MLPClassifier": "MLP",
        "GaussianNB": "GNB",
        "ComplementNB": "CNB",
        "AdaBoostClassifier": "AdaBoost",
        "KNeighborsClassifier": "KNN",
        "DecisionTreeClassifier": "DecisionTree",
        "BaggingClassifier": "Bagging",
        "GradientBoostingClassifier": 'GBDT',
    }
    # colors = x5.CB91_Grad_BP # 用于决策曲线的颜色

    timing_results = {} ## 用于存储时间信息

    str_time = (
        str(datetime.datetime.now().hour)
        + str(datetime.datetime.now().minute)
        + str(datetime.datetime.now().second)
    )
    random_number = random.randint(1, 100)
    str_time = str_time + str(random_number)

    list_name = [group]

    save_str=""
    plot_name_dict_save = {}  ##存储所有图片文件名
    result_model_save = {}  ##模型存储信息
    resThreshold = 0  #用于存储最终的模型最佳阈值
    df_save_dic={} # 用于存储最终结果数据表字典
    conf_dic_train, conf_dic_valid, conf_dic_test = {}, {}, {} # 用于存储置信区间
    df_input= df.copy(deep=True)

    #_____数据处理计时_____
    start_time_data_prep = time.time()

    if testSet:
        df = df[features + [group] + [label]].dropna()
        if label in features or label == group:
            return {"error": "标签列不能在所在模型中，请重新选择数据划分标签列！"+"false-error"}
    else: # 如果未指定测试集划分标签，只选择特征列和分组列
        df = df[features + [group]].dropna()

    binary = True
    u = np.sort(np.unique(np.array(df[group]))) #
    if len(u) == 2 and set(u) != set([0, 1]):
        y_result = label_binarize(df[group], classes=[ii for ii in u])
        y_result_pd = pd.DataFrame(y_result, columns=[group])
        df = pd.concat([df.drop(group, axis=1), y_result_pd], axis=1)
    elif len(u) > 2:
        if len(u) > 10:
            return {"error": "暂不允许类别数目大于10的情况。请检查因变量取值情况。"+"false-error"}
        binary = False
        if scoring == "roc_auc":
            scoring = scoring + "_ovo"
        else:
            scoring = scoring + "_macro"
        return {"error": "暂时只支持二分类。请检查因变量取值情况。"+"false-error"}

    # 数据划分逻辑
    if testSet:
        if isinstance(df[label][0], str):
            testLabel = str(testLabel)
            trainLabel = str(trainLabel)
        df = df[features + [group] + [label]].dropna()
        if datasave:
            df_save=df_input.iloc[list(df.index)]

        if isKFold == 'resample1' and trainSet:
            test_a = df[df[label] == testLabel]
            train_a = df[df[label] != testLabel]
            train_t = train_a[train_a[label] == trainLabel]
            valid_t = train_a[train_a[label] != trainLabel]
            train_alls = train_t.drop(label, axis=1)
            valid_alls = valid_t.drop(label, axis=1)
            test_alls = test_a.drop(label, axis=1)
            Xtrain = train_alls.drop(group, axis=1)
            Ytrain = train_alls.loc[:, list_name].squeeze(axis=1)
            X_valid = valid_alls.drop(group, axis=1)
            Y_valid = valid_alls.loc[:, list_name].squeeze(axis=1)
            # Xtest = test_a.drop(label, axis=1)
            Xtest = test_alls.drop(group, axis=1)
            Ytest = test_a.loc[:, list_name].squeeze(axis=1)

            if datasave:
                 df_save['Label_ML'] = list(map(lambda x: int(x), np.zeros(len(df_save))))
                 df_save.loc[list(Xtest.index), 'Label_ML'] = 2
                 df_save.loc[list(X_valid.index), 'Label_ML'] = 1
                 df_save.loc[list(Xtrain.index), 'Label_ML'] = 0
                 save_str += '在对新数据进行保存中，新的数据划分标签为：Label_ML，便签数据中的2为测试集，1为验证集，0为训练集。'
        else: # 如果指定了测试集划分标签，不是 resample1 模式或未指定训练集划分标签
            test_a = df[df[label] == testLabel]
            train_a = df[df[label] != testLabel]
            train_all = train_a.drop(label, axis=1)
            test_all = test_a.drop(label, axis=1)
            df = df.drop(label, axis=1)
            Xtrain = train_all.drop(group, axis=1)
            Ytrain = train_all.loc[:, list_name].squeeze(axis=1)
            Xtest = test_all.drop(group, axis=1)
            Ytest = test_all.loc[:, list_name].squeeze(axis=1)

            if datasave:
                df_save['Label_ML'] = list(map(lambda x: int(x), np.zeros(len(df_save))))
                df_save.loc[list(Xtest.index),'Label_ML'] = 1 # 测试集标签为 1
                save_str += '在对新数据进行保存中，新的数据划分标签为：Label_ML，便签数据中的1为测试集，0为训练集。'

    else:  # 如果未指定测试集划分标签 (随机划分训练集和测试集)
        df = df[features + [group]].dropna()
        X = df.drop(group, axis=1)
        Y = df.loc[:, list_name].squeeze(axis=1)
        X_train_temp, Xtest, Y_train_temp, Ytest = TTS(
            X,
            Y,
            test_size=test_ratio,
            random_state=randomState,
            stratify=Y
        )
        if isKFold == 'resample1' and trainSet:
            # 在临时训练集上再次分割，创建出最终的训练集和验证集
            val_size_adjusted = validation_ratio / (1.0 - test_ratio)
            Xtrain, X_valid, Ytrain, Y_valid = TTS(
            X_train_temp,
                   Y_train_temp,
                   test_size=val_size_adjusted,
                   random_state=randomState,
                   stratify=Y_train_temp
            )
        else:
            Xtrain, Ytrain = X_train_temp, Y_train_temp

        if datasave:
            df_save=df_input.iloc[list(df.index)]
            df_save['Label_ML'] = list(map(lambda x: int(x), np.zeros(len(df_save))))
            if isKFold == 'resample1': # 如果是 resample1 模式，测试集标签为 2
                df_save.loc[list(Xtest.index), 'Label_ML'] = 2
                save_str+='在对新数据进行保存中，新的数据划分标签为：Label_ML，便签数据中的2为测试集，1为验证集，0为训练集。'
            else:
                df_save.loc[list(Xtest.index),'Label_ML'] = 1
                save_str += '在对新数据进行保存中，新的数据划分标签为：Label_ML，便签数据中的1为测试集，0为训练集。'



    df_dict = {}

    str_result = "采用%s机器学习方法进行分类，分类变量为%s，模型中的变量包括" % (method, group)
    str_result += "、".join(features)

    #数据预处理计算结束
    end_time_data_prep = time.time()
    timing_results['数据预处理与划分时间'] = end_time_data_prep - start_time_data_prep

    #占位符
    str_result += "\n[TIMING_PLACEHOLDER]\n"

    plot_name_dict1 = {} # 用于存储寻参相关的图片文件名

    #计时超参数调优
    start_time_tuning = time.time()

    if searching == "auto":
            # 网格寻参和随机寻参方法重写
            if auto_model == "RandomizedSearchCV":# 随机寻参
                searcher = RandomizedSearchCV(globals()[method](), param_distributions=RandDefaultRange[method], random_state=randomState,n_jobs=-1,cv=n_splits) # 添加 random_state

            elif auto_model == "GridSearchCV":# 网格寻参
                searcher = GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method],n_jobs=-1,cv=n_splits)

            elif auto_model == "BayesSearchCV":# 贝叶斯寻参
                searcher = BayesSearchCV(globals()[method](), search_spaces=BayesDefaultRange[method], random_state=randomState,n_jobs=-1,n_iter=10,cv=n_splits) # 添加 random_state

            elif auto_model == "HalvingGridSearchCV":# 连续二分网格搜索
                searcher = HalvingGridSearchCV(globals()[method](), param_grid=GridDefaultRange[method], random_state=randomState,n_jobs=-1,cv=n_splits) # 添加 random_state

            elif auto_model == "HalvingRandomSearchCV":# 连续二分随机搜索
                searcher = HalvingRandomSearchCV(globals()[method](), param_distributions=RandDefaultRange[method], random_state=randomState,n_jobs=-1,cv=n_splits) # 添加 random_state

            elif auto_model == "GeneticSearchCV":  # 遗传算法寻参
                searcher = GeneticSearchCV(
                    globals()[method](),
                    param_grid=GeneticDefaultRange[method],
                    n_generations=10,
                    population_size=50,
                    mutation_rate=0.1,
                    crossover_rate=0.8,
                    elite_size=2,
                    scoring=scoring,
                    cv=n_splits,
                    random_state=randomState,
                    n_jobs = -1
                )
            elif auto_model == "Hyperopt":     # Hyperopt 算法寻参
                searcher = HyperoptSearchCV(
                    estimator=globals()[method](),
                    param_space=HyperDefaultRange[method],
                    max_evals=100,
                    scoring=scoring,
                    cv=n_splits,
                    random_state=randomState,
                    n_jobs = -1
                )

            searcher.fit(Xtrain, Ytrain)  # 执行寻参包含了交叉验证
            clf = searcher.best_estimator_
            print(clf)

            if auto_model == "Hyperopt":
                fig = searcher.plot_trials()
                str_time = str(datetime.datetime.now().hour) + str(datetime.datetime.now().minute) + str(
                    datetime.datetime.now().second)
                random_number = random.randint(1, 1000)
                str_time = str_time + str(random_number)
                savepath_temp = 'best_loss_plot' + str_time
                pic_name = savepath_temp + '.png'
                pic_name1 = savepath_temp + '.' + picFormat
                fig.savefig(savePath + pic_name, bbox_inches='tight', format='png', dpi=dpi)
                fig.savefig(savePath + pic_name1, bbox_inches='tight', format=picFormat, dpi=dpi)
                plt.close(fig)
                plot_name_dict1.update({'迭代最佳损失曲线（训练损失图）': pic_name})
                plot_name_dict_save.update({'迭代最佳损失曲线（训练损失图）': pic_name1})
            #最佳参数
            best_index = searcher.best_index_
            mean_valid_score = searcher.cv_results_['mean_test_score'][best_index]
            if 'std_test_score' in searcher.cv_results_:
                std_valid_score = searcher.cv_results_['std_test_score'][best_index]
            else:
                # 判断不存在，则将标准差设为 NaN
                std_valid_score = np.nan

            #从searcher中获取阈值
            _, _, full_train_metrics, _ = classification_metric_evaluate(clf, Xtrain, Ytrain, binary)
            resThreshold = full_train_metrics["cutoff"]
            # print(resThreshold)


    elif searching == "handle": # 如果手动指定参数
        if method == "SVC":
            kwargs["probability"] = True #
        if method == "RandomForestClassifier" and kwargs.get("max_depth") == "None":
            kwargs["max_depth"] = None
        if method == "DecisionTreeClassifier" and kwargs.get("max_depth") == "None":
            kwargs["max_depth"] = None
        if method == "MLPClassifier": # 神经网络参数处理
            hls_vals = str(kwargs["hidden_layer_sizes"]).split(",") # 获取隐藏层大小字符串并分割
            hls_value = () # 初始化隐藏层大小元组
            for hls_val in hls_vals:
                try:
                    if int(hls_val) >= 5 and int(hls_val) <= 200: # 检查隐藏层宽度范围
                        hls_value = hls_value + (int(hls_val),)
                    else:
                        return {"error": "请按照要求重新设置隐藏层宽度！"+"false-error"}
                except:
                    return {"error": "请重新设神经网络模型中的隐藏层宽度！"+"false-error"}
            kwargs["hidden_layer_sizes"] = hls_value
        if method == "GaussianNB" and kwargs.get("priors") == "None":
            kwargs["priors"] = None
        elif method == "GaussianNB":
            pri_vals = str(kwargs["priors"]).split(",")
            pri_value = ()
            pri_sum = 0.0
            for pri_val in pri_vals:
                try:
                    pri_sum = float(pri_val) + pri_sum
                    pri_value = pri_value + (float(pri_val),)
                except:
                    return {"error": "请重新设朴素贝叶斯模型中的先验概率！"+"false-error"}
            if len(pri_vals) == len(Y.unique()) and abs(pri_sum - 1.0) < 1e-6:
                kwargs["priors"] = pri_value
            else:
                return {"error": "请重新设朴素贝叶斯模型中的先验概率！"+"false-error"}
        if method in random_model:
            kwargs["random_state"]=42
        clf = globals()[method](**kwargs).fit(Xtrain, Ytrain)

    elif searching == "default": # 如果使用默认参数
        if method == "SVC":
            kwargs["probability"] = True
        elif method == "MLPClassifier":
            kwargs["hidden_layer_sizes"] = (20, 10)
            kwargs["max_iter"] = 20
        elif method == "RandomForestClassifier":
            kwargs["n_estimators"] = 20
        if method in random_model:
            kwargs["random_state"]=42
        clf = globals()[method](**kwargs).fit(Xtrain, Ytrain)

    if searching == 'auto':
        str_result += "\n选择的自动寻参方法为\n%s："+str(auto_model)
        str_result += "\n模型参数为:\n%s" % dic2str(searcher.best_params_, method) # 添加最佳参数到结果描述
    else: # 如果手动或使用默认参数
        str_result += "\n模型参数为:\n%s" % dic2str(clf.get_params(), clf.__class__.__name__) # 添加模型参数到结果描述
    str_result += "\n数据集样本数总计N=%d例，分类变量中包含的类别信息为：\n" % (df.shape[0]) # 添加数据集信息
    group_labels = df[group].unique()
    group_labels.sort()
    for lab in group_labels:
        n = sum(df[group] == lab)
        str_result += "\t 类别(" + str(lab) + ")：N=" + str(n) + "例\n" # 添加类别信息到结果描述

    #计时超参数调优
    end_time_tuning = time.time()
    timing_results['超参数调优及模型构建与训练时间'] = end_time_tuning - start_time_tuning


    # 学习曲线分割数
    if isKFold=='resample1':
        lc_splits=5
    else:
        lc_splits=n_splits
    plot_name_list = x5.plot_learning_curve(
        clf,
        Xtrain,
        Ytrain,
        cv=lc_splits,
        scoring=scoring,
        path=savePath,
        dpi=dpi,
        picFormat=picFormat,
    )
    plot_name_dict_save["学习曲线"] = plot_name_list[1]
    plot_name_list.pop(len(plot_name_list) - 1)
    #画校准曲线
    calibration_curve_name, _ = plot_calibration_curve(
        clf,
        Xtrain,
        Xtest,
        Ytrain,
        Ytest,
        name=name_dict[method],
        path=savePath,
        smooth=smooth,
        picFormat=picFormat,
        dpi=dpi,
    )
    plot_name_list.append(calibration_curve_name[0])
    plot_name_dict_save["校准曲线"] = calibration_curve_name[1]


    if binary:
        fig_roc_valid = plt.figure(figsize=(4, 4), dpi=dpi) # 验证集 ROC 曲线图
        # 画对角线
        plt.plot(
            [0, 1],
            [0, 1],
            linestyle="--",
            lw=1,
            color="r",
            alpha=0.8,
        )
        plt.grid(which="major", axis="both", linestyle="-.", alpha=0.08, color="grey") # 添加网格线

    data_all={}
    _, _, full_train_metrics, _ = classification_metric_evaluate(clf, Xtrain, Ytrain, binary)
    resThreshold = full_train_metrics["cutoff"]
    print(resThreshold)

    best_auc = 0.0
    tprs_train, tprs_valid = [], [] #
    fpr_train_alls, tpr_train_alls = [], []
    mean_fpr = np.linspace(0, 1, 100)
    list_evaluate_dic_train = make_class_metrics_dict()
    list_evaluate_dic_valid = make_class_metrics_dict()


    #计时验证
    start_time_cv = time.time()

    # 交叉验证/重采样逻辑
    if isKFold == 'cross' or isKFold == 'nest': # 交叉验证或嵌套交叉验证
        KF = StratifiedKFold(n_splits=n_splits, random_state=randomState, shuffle=True) # 分层 KFold 交叉验证
        for i, (train_index, valid_index) in enumerate(KF.split(Xtrain, Ytrain)): # 遍历各折
            X_train_fold, X_valid_fold = Xtrain.iloc[train_index], Xtrain.iloc[valid_index] # 获取当前折的训练集和验证集特征
            Y_train_fold, Y_valid_fold = Ytrain.iloc[train_index], Ytrain.iloc[valid_index] # 获取当前折的训练集和验证集目标变量
            data_all.update({ # 存储当前折的数据
                i: {"Xtrain": X_train_fold,"Ytrain": Y_train_fold,"Xvalid": X_valid_fold,"Yvalid": Y_valid_fold}}
            )

            if isKFold == "nest": # 嵌套交叉验证
                best_auc_inner = 0.0
                resThreshold_inner = 0
                inner_cv = StratifiedKFold(
                    n_splits=n_splits, random_state=randomState, shuffle=True
                )
                for j, (train_index_inner, test_index_inner) in enumerate(inner_cv.split(X_train_fold, Y_train_fold)): # 遍历内层各折
                    X_train_inner, X_test_inner = (X_train_fold.iloc[train_index_inner],X_train_fold.iloc[test_index_inner],)
                    y_train_inner, y_test_inner = (Y_train_fold.iloc[train_index_inner],Y_train_fold.iloc[test_index_inner],)
                    model_i = clone(clf).fit(X_train_inner, y_train_inner) # 在内层训练集上训练模型
                    _, _, metric_dic_train_i, _ = classification_metric_evaluate( # 计算内层训练集指标
                        model_i, X_train_inner, y_train_inner, binary
                    )
                    _, _, metric_dic_valid_i, _ = classification_metric_evaluate( # 计算内层测试集指标 (作为验证)
                        model_i,
                        X_test_inner,
                        y_test_inner,
                        binary,
                        Threshold=metric_dic_train_i["cutoff"], # 使用内层训练集阈值
                    )
                    if metric_dic_valid_i["AUC"] > best_auc_inner:
                        best_auc_inner = metric_dic_valid_i["AUC"]
                        model_inner = model_i # 更新内层最佳模型
                        resThreshold_inner = metric_dic_train_i["cutoff"]
                        data_all[i].update({'model':model_inner,'threshold':resThreshold_inner})

    else: # 如果是重采样模式 ('resample' 或 'resample1')
        if isKFold=='resample1' and trainSet: # 如果是 resample1 模式且指定了训练集划分标签
             data_all.update({
                 0: {"Xtrain": Xtrain, "Ytrain": Ytrain, "Xvalid": X_valid, "Yvalid": Y_valid}}
             )
             # 在 resample1 模式且 trainSet 为 True 的情况下，需要在这里训练模型
             model = clone(clf).fit(Xtrain, Ytrain)
             data_all[0].update({'model': model, 'threshold': 0.5}) # 存储模型并设置默认阈值 0.5

        else: # 如果是 'resample' 模式 或 ('resample1' 且未指定 trainSet)
            for i in range(n_splits): # 进行 n_splits 次重采样
                X_train_rs, X_valid_rs, Y_train_rs, Y_valid_rs = TTS(Xtrain, Ytrain, test_size=validation_ratio, random_state=randomState + i) # 随机划分训练集和验证集
                data_all.update({ # 存储当前重采样的数据
                    i: {"Xtrain": X_train_rs, "Ytrain": Y_train_rs, "Xvalid": X_valid_rs, "Yvalid": Y_valid_rs}}
                )
            if isKFold == 'resample1' and not testSet and datasave: #resample1 模式且未指定测试集划分标签，需要保存数据
                 df_save.loc[list(data_all[0]['Xvalid'].index), 'Label_ML'] = 1

    i=0
    for data_key, data_value in data_all.items():
        X_train_iter, Y_train_iter, X_valid_iter, Y_valid_iter = (data_value["Xtrain"],data_value["Ytrain"],data_value["Xvalid"],data_value["Yvalid"],) # 获取当前折/重采样的数据

        if isKFold!='nest':
            if not (isKFold == 'resample1' and trainSet):
                 model = clone(clf).fit(X_train_iter, Y_train_iter) # 克隆并训练模型
            fpr_train, tpr_train, metric_dic_train, _ = classification_metric_evaluate(model, X_train_iter, Y_train_iter, binary) # 计算训练集指标
        else:
            model = data_all[data_key]['model']
            fpr_train, tpr_train, metric_dic_train, _ = classification_metric_evaluate( # 计算训练集指标，使用内层最佳阈值
                model, X_train_iter, Y_train_iter, binary, Threshold=data_all[data_key]['threshold'])

        fpr_valid, tpr_valid, metric_dic_valid, _ = classification_metric_evaluate( # 计算验证集指标
            model, X_valid_iter, Y_valid_iter, binary, Threshold=metric_dic_train["cutoff"] # 使用训练集计算出的阈值
        )
        metric_dic_valid.update({"cutoff": metric_dic_train["cutoff"]})

        # 计算所有评价指标并添加到列表中
        for key in list_evaluate_dic_train.keys():
            list_evaluate_dic_train[key].append(metric_dic_train[key])
            list_evaluate_dic_valid[key].append(metric_dic_valid[key])

        if binary:

            tprs_valid.append(np.interp(mean_fpr, fpr_valid, tpr_valid))
            tprs_valid[-1][0] = 0.0

            # 绘制验证集 ROC 曲线
            if validationCurve:
                plt.figure(fig_roc_valid.number)
                plt.plot(
                    fpr_valid,
                    tpr_valid,
                    lw=1,
                    alpha=0.4,
                    color=palette_dict[style][i % len(palette_dict[style])],
                    label="ROC fold %4d (auc=%0.3f 95%%CI (%0.3f-%0.3f))"
                    % (
                        i + 1,
                        metric_dic_valid["AUC"],
                        metric_dic_valid["AUC_L"],
                        metric_dic_valid["AUC_U"],
                    ) if resultType == 1 else "ROC fold %4d auc=%0.3f" % (i+1, metric_dic_valid["AUC"]),
                )

            #训练集 ROC 曲线
            fpr_train_alls.append(fpr_train)
            tpr_train_alls.append(tpr_train)
            tprs_train.append(np.interp(mean_fpr, fpr_train, tpr_train))
            tprs_train[-1][0] = 0.0

        i+=1

    #验证
    end_time_cv = time.time()
    timing_results['验证时间（交叉验证/嵌套交叉验证/重采样）'] = end_time_cv - start_time_cv

    if modelSave:
        import pickle

        modelfile = open(savePath + method + str_time + ".pkl", "wb")
        pickle.dump(clf, modelfile)
        modelfile.close()
        result_model_save["modelFile"] = method + str_time + ".pkl"
        result_model_save["modelFeature"] = features
    if datasave:
        res_pro = clf.predict_proba(df_save[features])
        feas_Yprob = []
        for i in range(res_pro.shape[1]):
            feas_Yprob.append('Yprob_' + str(i)+'_'+str_time)

        pd_Yprob = pd.DataFrame(res_pro, columns=feas_Yprob, index=df_save.index)
        df_save = pd.concat([df_save, pd_Yprob], axis=1)
        df_save['Threshold'+'_'+str_time] = resThreshold
        df_dict.update({'存储数据表':df_save})
        str_result += '\n' + save_str


    if binary:
        mean_tpr_valid = np.mean(tprs_valid, axis=0)
        mean_tpr_valid[-1] = 1.0
        mean_auc_valid = np.mean(list_evaluate_dic_valid["AUC"])
        aucs_lower, aucs_upper = ci(list_evaluate_dic_valid["AUC"])
        plt.figure(fig_roc_valid.number)
        plt.plot(
            mean_fpr,
            mean_tpr_valid,
            color=palette_dict[style][0],
            lw=2,
            alpha=0.8,
            label=r"Mean (validation) ROC (auc=%0.3f 95%%CI (%0.3f-%0.3f))"
            % (
                mean_auc_valid,
                np.mean(list_evaluate_dic_valid["AUC_L"]),
                np.mean(list_evaluate_dic_valid["AUC_U"]),
            ) if resultType == 1 else r"Mean (validation) ROC (auc=%0.3f SD (%0.3f))" % (mean_auc_valid, np.std(list_evaluate_dic_valid["AUC"])), # 根据 resultType 选择标签格式 (SD)
        )
        plt.xlim([-0.02, 1.02])
        plt.ylim([-0.02, 1.02])
        plt.xlabel("1-Specificity")
        plt.ylabel("Sensitivity")
        plt.title("ROC curve(Validation)")
        plt.legend(loc="lower right", fontsize=5)
        if savePath is not None:
            plot_name_list.append(
                save_fig(savePath, "ROC_curve", "png", fig_roc_valid, str_time=str_time)
            )
            plot_name_dict_save["验证集 ROC 曲线"] = save_fig(
                savePath, "ROC_curve", picFormat, fig_roc_valid, str_time=str_time
            )
        plt.close(fig_roc_valid)


    mean_dic_train, stdv_dic_train = {}, {}
    mean_dic_valid, stdv_dic_valid = {}, {}
    for key in list_evaluate_dic_valid.keys():
        mean_dic_train[key] = np.mean(list_evaluate_dic_train[key])
        mean_dic_valid[key] = np.mean(list_evaluate_dic_valid[key])
        if resultType == 0:
            stdv_dic_train[key] = np.std(list_evaluate_dic_train[key], axis=0)
            stdv_dic_valid[key] = np.std(list_evaluate_dic_valid[key], axis=0)
        elif resultType == 1:
            conf_dic_train[key] = list(ci(list_evaluate_dic_train[key]))
            conf_dic_valid[key] = list(ci(list_evaluate_dic_valid[key]))

    resThreshold_train = mean_dic_train['cutoff']

    (
        fpr_test,
        tpr_test,
        metric_dic_test,
        df_test_result,
    ) = classification_metric_evaluate( # 计算测试集指标
        clf, Xtest, Ytest, binary, Threshold=resThreshold # 使用最佳模型和阈值
    )
    metric_dic_test.update({"cutoff": resThreshold})

    # 混淆矩阵表格(测试集)
    from sklearn.metrics import confusion_matrix
    y_pred = clf.predict_proba(Xtest)[:, 1] # 获取测试集预测概率
    y_pred[y_pred > resThreshold] = 1
    y_pred[y_pred <= resThreshold] = 0
    cm = confusion_matrix(Ytest, y_pred) # 计算混淆矩阵(测试集)
    cm_df = pd.DataFrame(
        cm, index=['真实值: ' + str(group_labels[0]), '真实值: ' + str(group_labels[1])],
        columns=['预测值: ' + str(group_labels[0]), '预测值: ' + str(group_labels[1])]
    ).T # 转置数据框
    cm_df['Freq'] = cm_df.sum(axis=1)
    cm_df = cm_df.reset_index().rename(columns={'index': '预测值'})


    # 混淆矩阵表格(训练集)
    train_cutoff_numeric = np.mean(list_evaluate_dic_train["cutoff"])
    y_pred_train = clf.predict_proba(Xtrain)[:, 1]
    y_pred_train_classified = (y_pred_train > train_cutoff_numeric).astype(int)
    cm_train = confusion_matrix(Ytrain, y_pred_train_classified)
    cm_df_train = pd.DataFrame(
        cm_train, index=['真实值: ' + str(group_labels[0]), '真实值: ' + str(group_labels[1])],
        columns=['预测值: ' + str(group_labels[0]), '预测值: ' + str(group_labels[1])]
    ).T
    cm_df_train['Freq'] = cm_df_train.sum(axis=1)
    cm_df_train = cm_df_train.reset_index().rename(columns={'index': '预测值'})


    #画训练集 ROC
    if binary: #
        fig_roc_train = plt.figure(figsize=(4, 4), dpi=dpi)
        plt.plot(
            [0, 1],
            [0, 1],
            linestyle="--",
            lw=1,
            color="r",
            alpha=0.8,
        )
        plt.grid(which="major", axis="both", linestyle="-.", alpha=0.08, color="grey")

        if validationCurve:
            for i in range(len(tpr_train_alls)):
                plt.plot(
                    fpr_train_alls[i],
                    tpr_train_alls[i],
                    lw=1,
                    color=palette_dict[style][i % len(palette_dict[style])],
                    alpha=0.4,
                    label="ROC fold %4d (auc=%0.3f 95%%CI (%0.3f-%0.3f)) "
                    % (
                        i + 1,
                        list_evaluate_dic_train["AUC"][i],
                        list_evaluate_dic_train["AUC_L"][i],
                        list_evaluate_dic_train["AUC_U"][i],
                    ) if resultType == 1 else "ROC fold %4d auc=%0.3f " % (i+1, list_evaluate_dic_train["AUC"][i]),
                )

        mean_tpr_train = np.mean(tprs_train, axis=0)
        mean_tpr_train[-1] = 1.0
        mean_auc_train = np.mean(list_evaluate_dic_train["AUC"])
        plt.plot(
            mean_fpr,
            mean_tpr_train,
            color=palette_dict[style][0],
            lw=1.8,
            alpha=0.7,
            label=r"Mean (train) ROC (auc=%0.3f 95%%CI (%0.3f-%0.3f))"
            % (
                mean_auc_train,
                np.mean(list_evaluate_dic_train["AUC_L"]),
                np.mean(list_evaluate_dic_train["AUC_U"]),
            ) if resultType == 1 else "Mean (train) ROC (auc=%0.3f SD (%0.3f))" % (mean_auc_train,np.std(list_evaluate_dic_train["AUC"])), # 根据 resultType 选择标签格式 (SD)
        )
        plt.xlim([-0.02, 1.02])
        plt.ylim([-0.02, 1.02])
        plt.xlabel("1-Specificity")
        plt.ylabel("Sensitivity")
        plt.title("ROC curve(Training)")
        plt.legend(loc="lower right", fontsize=5)
        if savePath is not None:
            plot_name_list.append(
                save_fig(savePath, "ROC_curve_train", "png", fig_roc_train, str_time=str_time)
            )
            plot_name_dict_save["训练集 ROC 曲线"] = save_fig( # 保存指定格式图片
                savePath, "ROC_curve_train", picFormat, fig_roc_train, str_time=str_time
            )
        plt.close(fig_roc_train)


        plot_name_list.reverse()

        #画测试集 ROC
        fig_roc_test = plt.figure(figsize=(4, 4), dpi=dpi)
        plt.plot(
            [0, 1],
            [0, 1],
            linestyle="--",
            lw=1,
            color="r",
            alpha=0.8,
        )
        plt.grid(which="major", axis="both", linestyle="-.", alpha=0.08, color="grey")

        if smooth: # 如果需要平滑处理
            from scipy.interpolate import interp1d

            tpr_test_unique, tpr_test_index = np.unique(fpr_test, return_index=True)
            fpr_test_new = np.linspace(min(fpr_test), max(fpr_test), len(fpr_test))
            f = interp1d(
                tpr_test_unique, tpr_test[tpr_test_index], kind="linear"
            )
            tpr_test_new = f(fpr_test_new)
        else:
            fpr_test_new = fpr_test
            tpr_test_new = tpr_test
        plt.plot( # 绘制测试集 ROC 曲线
            fpr_test_new,
            tpr_test_new,
            lw=1.5,
            alpha=0.6,
            color=palette_dict[style][0],
            label="Test Set ROC (auc=%0.3f 95%%CI (%0.3f-%0.3f)) " # 标签格式
            % (
                metric_dic_test["AUC"],
                metric_dic_test["AUC_L"],
                metric_dic_test["AUC_U"],
            ) if resultType == 1 else "Test Set ROC auc=%0.3f" % (metric_dic_test["AUC"]),
        )
        plt.xlim([-0.02, 1.02])
        plt.ylim([-0.02, 1.02])
        plt.xlabel("1-Specificity")
        plt.ylabel("Sensitivity")
        plt.title("ROC curve(Test)")
        plt.legend(loc="lower right", fontsize=5)
        if savePath is not None:
            plot_name_list.append(
                save_fig(savePath, "ROC_curve_test", "png", fig_roc_test, str_time=str_time)
            )
            plot_name_dict_save["测试集 ROC 曲线"] = save_fig(
                savePath, "ROC_curve_test", picFormat, fig_roc_test, str_time=str_time
            )
        plt.close(fig_roc_test)

        if testSet:
            df_count_c = Xtest.shape[0]
            df_count_r = (Xtest.shape[0] / df.shape[0]) * 100
            if isKFold == 'resample1':
                str_result += "其中在总体样本中根据标签%s为%s划分为测试集，标签%s为训练集，其余标签归为验证集。其中" % (
                    label, testLabel, trainLabel)
            else:
                str_result += "其中在总体样本中根据标签%s为%s划分为总体测试集，其余标签归为训练集。其中" % (label,
                                                                                                          testLabel)
        else:
            df_count_c = Ytest.shape[0]
            df_count_r = (Ytest.shape[0] / df.shape[0]) * 100
            str_result += "其中在总体样本中随机抽取"
        diff, ratio = 0, 0
        if isKFold == "cross" or isKFold == 'nest':
            str_splits = '剩余样本作为训练集进行%d折交叉验证，' % n_splits
        elif isKFold == 'resample':
            valid_count = round(Xtrain.shape[0] * validation_ratio)#计算验证集数量
            str_splits = '剩余样本每次随机抽取验证集N=%d例(%d%%)作为验证数据进行%d次重采样，' % (valid_count,validation_ratio * 100, n_splits)
        elif isKFold == 'resample1':
            if not testSet:
                valid_count = round(Xtrain.shape[0] * validation_ratio)#计算验证集数量
                str_splits = '剩余样本随机抽取验证集N=%d例(%d%%)作为验证数据进行%d次重采样，' % (valid_count, validation_ratio * 100, n_splits)
            else:
                if trainSet:  # 仅在 trainSet 为 True 时执行
                    train_count = Xtrain.shape[0]
                    valid_count = X_valid.shape[0]
                    str_splits = "训练集N=%d例，验证集N=%d例" % (train_count, valid_count)
                else:
                    str_splits = "，"
        if resultType == 1:  ##CI
            str_result += (
                    "测试集N=%d例(%3.2f%%)，%s并在验证集中得到AUC=%5.4f(%5.4f-%5.4f)。\n最终模型在测试集中的AUC=%5.4f，准确度=%5.4f。\n"
                    % (
                        df_count_c,
                        df_count_r,
                        str_splits,
                        mean_dic_valid["AUC"],
                        mean_dic_valid["AUC_L"],
                        mean_dic_valid["AUC_U"],
                        df_test_result["AUC"].values[0],
                        df_test_result["准确度"].values[0],
                    )
            )
            diff = mean_dic_valid["AUC"] - float(df_test_result.loc["Mean", "AUC"])
            ratio = diff / float(df_test_result.loc["Mean", "AUC"])
        elif resultType == 0:  ##SD
            str_result += (
                    "取测试集N=%d例(%3.2f%%)，%s，并在验证集中得到AUC=%5.4f±%5.4f。\n最终模型在测试集中的AUC=%5.4f，准确度=%5.4f。\n"
                    % (
                        df_count_c,
                        df_count_r,
                        str_splits,
                        mean_dic_valid["AUC"],
                        stdv_dic_valid["AUC"],
                        df_test_result["AUC"].values[0],
                        df_test_result["准确度"].values[0],
                    )
            )
            diff = float(stdv_dic_valid["AUC"]) - float(df_test_result.loc["Mean", "AUC"])
            ratio = diff / float(df_test_result.loc["Mean", "AUC"])

        if not np.isnan(float(diff)) and diff > 0 and (ratio > 0.1):
            str_result += "注意到AUC指标下验证集表现超出测试集{}，约{}%，可能存在过拟合现象。建议更换模型或重新设置参数。".format(
                round(diff, decimal_num), round(ratio * 100, decimal_num)
            )
        else:
            str_result += (
                "鉴于AUC指标下验证集表现未超出测试集或超出比小于10%，可认为拟合成功，{}模型可以用于此数据集的分类建模任务。".format(
                    name_dict[method]
                )
            )
        str_result += "\n需注意：在结果表格中训练集结果汇总和验证集结果汇总是基于验证方式进行计算的，而测试集结果汇总是基于完整测试集进行的验证。"
        str_result += "\n如果想进一步对比更多分类模型的表现，可使用左侧栏智能分析中的‘分类多模型综合分析’功能。在混淆矩阵表中如果样本总数与分析结果出现细微偏差是由于使用四舍五入计算导致，不影响实际结果。"

        df_test_result = df_test_result.applymap(lambda x: round_dec(x, d=decimal_num))


    if resultType == 1:
        for tem in ["AUC", "AUC_L", "AUC_U"]:
             if tem in mean_dic_train and tem != "AUC": del mean_dic_train[tem]
             if tem in mean_dic_valid and tem != "AUC": del mean_dic_valid[tem]
             if tem in conf_dic_train and tem != "AUC": del conf_dic_train[tem]
             if tem in conf_dic_valid and tem != "AUC": del conf_dic_valid[tem]


        for key in conf_dic_train.keys():
            mean_dic_train[key] = (
                str(round_dec(float(mean_dic_train[key]), d=decimal_num))
                + "("
                + str(round_dec(float(conf_dic_train[key][0]), d=decimal_num))
                + "-"
                + str(round_dec(float(conf_dic_train[key][1]), d=decimal_num))
                + ")"
            )
            mean_dic_valid[key] = (
                str(round_dec(float(mean_dic_valid[key]), d=decimal_num))
                + "("
                + str(round_dec(float(conf_dic_valid[key][0]), d=decimal_num))
                + "-"
                + str(round_dec(float(conf_dic_valid[key][1]), d=decimal_num))
                + ")"
            )

        df_train_result = pd.DataFrame([mean_dic_train], index=["Mean"])
        df_valid_result = pd.DataFrame([mean_dic_valid], index=["Mean"])

        df_train_result.rename(
            columns={
                "AUC": "AUC(95%CI)",
                "cutoff": "cutoff(95%CI)",
                "准确度": "准确度(95%CI)",
                "灵敏度": "灵敏度(95%CI)",
                "特异度": "特异度(95%CI)",
                "阳性预测值": "阳性预测值(95%CI)",
                "阴性预测值": "阴性预测值(95%CI)",
                "F1分数": "F1分数(95%CI)",
                "Kappa": "Kappa(95%CI)",
            },
            inplace=True,
        )
        df_valid_result.rename(
            columns={
                "AUC": "AUC(95%CI)",
                "cutoff": "cutoff(95%CI)",
                "准确度": "准确度(95%CI)",
                "灵敏度": "灵敏度(95%CI)",
                "特异度": "特异度(95%CI)",
                "阳性预测值": "阳性预测值(95%CI)",
                "阴性预测值": "阴性预测值(95%CI)",
                "F1分数": "F1分数(95%CI)",
                "Kappa": "Kappa(95%CI)",
            },
            inplace=True,
        )

        df_test_result["AUC (95%CI)"] = df_test_result["AUC"].astype(str) + " (" + df_test_result["AUC_L"].astype(str) + "-" + df_test_result["AUC_U"].astype(str) + ")"
        df_test_result.drop(columns=["AUC", "AUC_L", "AUC_U"], inplace=True)
        all_columns = df_test_result.columns.tolist()
        new_order = ['AUC (95%CI)'] + [col for col in all_columns if col != 'AUC (95%CI)']
        df_test_result = df_test_result[new_order]


    elif resultType == 0:
        for tem in ["AUC_L", "AUC_U"]:
            if tem in mean_dic_train: del mean_dic_train[tem]
            if tem in mean_dic_valid: del mean_dic_valid[tem]
            if tem in stdv_dic_train: del stdv_dic_train[tem]
            if tem in stdv_dic_valid: del stdv_dic_valid[tem]

        for key in stdv_dic_train.keys():
            mean_dic_train[key] = (
                str(round_dec(float(mean_dic_train[key]), d=decimal_num))
                + " ("
                + str(round_dec(float(stdv_dic_train[key]), d=decimal_num))
                + ")"
            )
            mean_dic_valid[key] = (
                str(round_dec(float(mean_dic_valid[key]), d=decimal_num))
                + " ("
                + str(round_dec(float(stdv_dic_valid[key]), d=decimal_num))
                + ")"
            )

        df_train_result = pd.DataFrame([mean_dic_train], index=["Mean"])
        df_valid_result = pd.DataFrame([mean_dic_valid], index=["Mean"])

        df_train_result.rename(
            columns={
                "AUC": "AUC(SD)",
                "cutoff": "cutoff(SD)",
                "准确度": "准确度(SD)",
                "灵敏度": "灵敏度(SD)",
                "特异度": "特异度(SD)",
                "阳性预测值": "阳性预测值(SD)",
                "阴性预测值": "阴性预测值(SD)",
                "F1分数": "F1分数(SD)",
                "Kappa": "Kappa(SD)",
            },
            inplace=True,
        )
        df_valid_result.rename(
            columns={
                "AUC": "AUC(SD)",
                "cutoff": "cutoff(SD)",
                "准确度": "准确度(SD)",
                "灵敏度": "灵敏度(SD)",
                "特异度": "特异度(SD)",
                "阳性预测值": "阳性预测值(SD)",
                "阴性预测值": "阴性预测值(SD)",
                "F1分数": "F1分数(SD)",
                "Kappa": "Kappa(SD)",
            },
            inplace=True,
        )
        df_test_result.drop(columns=["AUC_L", "AUC_U"], inplace=True)


    df_dictjq = { # 结果数据表字典
        "训练集结果汇总": df_train_result,
        "验证集结果汇总": df_valid_result,
        "测试集结果汇总": df_test_result,
        "测试集混淆矩阵": cm_df, # 混淆矩阵数据框
        "训练集混淆矩阵": cm_df_train,

    }
    df_dict.update(df_dictjq)

    plot_name_dict = {
        "训练集 ROC 曲线图": plot_name_list[0],
        "验证集 ROC 曲线图": plot_name_list[1],
        "测试集 ROC 曲线图": plot_name_list[4],
        "学习曲线图": plot_name_list[3],
        "模型校准曲线": plot_name_list[2],
    }

    if binary:  #画 DCA 曲线
        DCA_dict = {}
        (
            prob_pos,
            p_serie,
            net_benefit_serie,
            net_benefit_serie_All,
        ) = calculate_net_benefit(clf, Xtest, Ytest) # 计算净获益
        DCA_dict[name_dict[method]] = { # 存储当前模型的 DCA 结果
            "p_serie": p_serie,
            "net_b_s": net_benefit_serie,
            "net_b_s_A": net_benefit_serie_All,
        }
        decision_curve_p = plot_decision_curves( # 绘制决策曲线
            DCA_dict,
            colors=palette_dict[style],
            name="Test",
            savePath=savePath,
            dpi=dpi,
            picFormat=picFormat,
            DCA_cut=DCA_cut,
        )
        plot_name_dict["测试集 DCA 曲线图"] = decision_curve_p[0]
        plot_name_dict_save["测试集 DCA 曲线图"] = decision_curve_p[1]


    #画混淆矩阵图
    if isKFold =='resample1' and trainSet: # 如果是 resample1 模式且指定了训练集划分标签
        pic_matrix,pic_save_matrix=_plot_conffusion_matrix(clf, Xtrain, Ytrain, group_labels,resThreshold=resThreshold_train,
                                savePath=savePath,picFormat=picFormat, dpi=600)
        plot_name_dict.update(pic_matrix)
        plot_name_dict_save.update(pic_save_matrix)
        pic_matrix, pic_save_matrix = _plot_conffusion_matrix(clf, X_valid, Y_valid, group_labels,resThreshold=resThreshold_train,
                                                              name='valid_confusionMatrix_',
                                                              dic_name='最优模型验证集混淆矩阵', str_time=str_time,
                                                              savePath=savePath, picFormat=picFormat, dpi=600)
        plot_name_dict.update(pic_matrix)
        plot_name_dict_save.update(pic_save_matrix)
    else: # 包括 cross, nest, resample 模式，以及 resample1 模式但未指定 trainSet
        pic_matrix, pic_save_matrix = _plot_conffusion_matrix(clf, Xtrain, Ytrain, group_labels,resThreshold=resThreshold_train,
                                                              name='train_confusionMatrix_',
                                                              dic_name='最优模型训练集混淆矩阵', str_time=str_time,
                                                              savePath=savePath, picFormat=picFormat, dpi=600)
        plot_name_dict.update(pic_matrix)
        plot_name_dict_save.update(pic_save_matrix)

    # 测试集混淆矩阵图
    pic_matrix, pic_save_matrix = _plot_conffusion_matrix(clf, Xtest, Ytest, group_labels,resThreshold=resThreshold,
                                                          name='test_confusionMatrix_',
                                                          dic_name='最优模型测试集混淆矩阵', str_time=str_time,
                                                          savePath=savePath, picFormat=picFormat, dpi=600)
    plot_name_dict.update(pic_matrix)
    plot_name_dict_save.update(pic_save_matrix)

    # KS 曲线图
    import scikitplot as skplt
    plot_name = 'KS_' + str_time + ".png"
    plot_name_save = 'KS_' + str_time + "." + picFormat
    y_probas = clf.predict_proba(Xtest)
    skplt.metrics.plot_ks_statistic(Ytest, y_probas, title='KS Statistic Plot(Test)')
    plt.savefig(savePath + plot_name, dpi=dpi, format='png', bbox_inches='tight')
    plt.savefig(savePath + plot_name_save, dpi=dpi, format=picFormat, bbox_inches='tight')
    plt.close() # 关闭图形
    plot_name_dict.update({'测试集 KS 曲线': plot_name})
    plot_name_dict_save.update({'测试集 KS 曲线': plot_name_save})

    # 画 parallel_coordinates 图
    from yellowbrick.features import parallel_coordinates
    try:
        visualizer = parallel_coordinates(df[features], df[group], normalize=None)
        plot_name = "parallel_coordinates_" + str_time + ".png"
        plot_name_save = "parallel_coordinates_" + str_time + "." + picFormat
        pic_kwargs = {'dpi': dpi, 'format': 'png', 'bbox_inches': 'tight'}
        visualizer.show(outpath=savePath + plot_name, clear_figure=True, **pic_kwargs)
        pic_kwargs = {'dpi': dpi, 'format': picFormat, 'bbox_inches': 'tight'}
        plt.close()
        visualizer = parallel_coordinates(df[features], df[group], normalize=None)
        visualizer.show(outpath=savePath + plot_name_save,clear_figure=True, **pic_kwargs)
        plot_name_dict.update({'全部数据_parallel_coordinates图': plot_name})
        plot_name_dict_save.update({'全部数据_parallel_coordinates图': plot_name_save})
        plt.close()
    except Exception as e:
        print(f"Error plotting parallel coordinates: {e}")


    # 决策树图
    if method == 'DecisionTreeClassifier' and len(features) < 10:
        try:
            import pydotplus #
            from sklearn import tree #
            from sklearn.tree import export_graphviz
            plot_name = "DecisionTree_pic_" + str_time + ".png"
            plot_name_save = "DecisionTree_pic_" + str_time + "." + picFormat
            group_u=list(np.sort(np.unique(np.array(df[group]))))
            group_name = [str(num) for num in group_u]

            model_for_plot = clf
            if searching == "auto":
                if hasattr(searcher, 'best_estimator_'):
                    model_for_plot = searcher.best_estimator_

            dot_data = tree.export_graphviz(model_for_plot, out_file=None,
                                            feature_names=features,
                                            class_names=np.array(group_name),
                                            filled=True, rounded=True,
                                            special_characters=True)

            graph = pydotplus.graph_from_dot_data(dot_data)
            graph.write_png(savePath+plot_name)
            if picFormat == 'pdf':
                graph.write_pdf(savePath+plot_name_save)
            else:
                graph.write_png(savePath + plot_name_save)
            plot_name_dict.update({'决策树图': plot_name})
            plot_name_dict_save.update({'决策树图': plot_name_save})
        except Exception as e:
            print(f"Error plotting Decision Tree: {e}")


    if explain or modelSave:
        import shap

        f = lambda x: clf.predict_proba(x)[:, 1]
        med = Xtrain.median().values.reshape((1, Xtrain.shape[1]))

        result_model_save["modelShapValue"] = [ # 存储 SHAP 解释所需的参考值（通常是中位数）
            float("{:.3f}".format(i)) for i in list(med[0])
        ]
        result_model_save["modelName"] = method
        result_model_save["modelClass"] = "机器学习分类"
        result_model_save["Threshold"] = resThreshold

    df_shapValue = Xtest # 用于 SHAP 解释的数据，默认为测试集
    df_shapValue_show = pd.DataFrame()
    shapValue_list = []
    shapValue_name = []
    if explain:
        if shapSet == 2:  ## Xtrain, Xtest, Ytrain, Ytest 中选择 SHAP 样本
            df_shapValue = Xtest # SHAP 解释数据为测试集
            if explain_sample == 4: # 解释 4 个代表性样本
                flage1, flage2, flage3, flage4 = True, True, True, True
                for i in range(len(Ytest)):
                    if ( # 预测为 1 实际为 1 的样本
                        flage1
                        and f(df_shapValue.iloc[i : i + 1, :])[0] >= resThreshold
                        and Ytest.iloc[i,] == 1
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_list.append(i)
                        shapValue_name.append("shap_样本_预测值为1实际值为1")
                        flage1 = False
                    elif ( # 预测为 1 实际为 0 的样本
                        flage2
                        and f(df_shapValue.iloc[i : i + 1, :])[0] >= resThreshold
                        and Ytest.iloc[i,] == 0
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_list.append(i)
                        shapValue_name.append("shap_样本_预测值为1实际值为0")
                        flage2 = False
                    elif ( # 预测为 0 实际为 1 的样本
                        flage3
                        and f(df_shapValue.iloc[i : i + 1, :])[0] < resThreshold
                        and Ytest.iloc[i,] == 1
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为0实际值为1")
                        shapValue_list.append(i)
                        flage3 = False
                    elif ( # 预测为 0 实际为 0 的样本
                        flage4
                        and f(df_shapValue.iloc[i : i + 1, :])[0] < resThreshold
                        and Ytest.iloc[i,] == 0
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为0实际值为0")
                        shapValue_list.append(i)
                        flage4 = False

                    if not flage1 and not flage2 and not flage3 and not flage4:
                        break
            else:
                df_shapValue_show = pd.concat(
                    [df_shapValue_show, df_shapValue.iloc[0:explain_sample, :]], axis=0
                )
                shapValue_list.extend(i for i in range(explain_sample))
                shapValue_name.extend(
                    "shap_样本_" + str(i) for i in range(explain_sample)
                )

        elif shapSet == 1: # 在 Xtrain 中选择 SHAP 样本
            df_shapValue = Xtrain
            if explain_sample == 4:
                flage1, flage2, flage3, flage4 = True, True, True, True
                for i in range(len(Ytrain)):
                    if (
                        flage1
                        and f(df_shapValue.iloc[i : i + 1, :])[0] >= resThreshold
                        and Ytrain.iloc[i,] == 1
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为1实际值为1")
                        shapValue_list.append(i)
                        flage1 = False
                    elif (
                        flage2
                        and f(df_shapValue.iloc[i : i + 1, :])[0] >= resThreshold
                        and Ytrain.iloc[i,] == 0
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为1实际值为0")
                        shapValue_list.append(i)
                        flage2 = False
                    elif (
                        flage3
                        and f(df_shapValue.iloc[i : i + 1, :])[0] < resThreshold
                        and Ytrain.iloc[i,] == 1
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为0实际值为1")
                        shapValue_list.append(i)
                        flage3 = False
                    elif (
                        flage4
                        and f(df_shapValue.iloc[i : i + 1, :])[0] < resThreshold
                        and Ytrain.iloc[i,] == 0
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为0实际值为0")
                        shapValue_list.append(i)
                        flage4 = False

                    if not flage1 and not flage2 and not flage3 and not flage4:
                        break
            else:
                df_shapValue_show = pd.concat(
                    [df_shapValue_show, df_shapValue.iloc[0:explain_sample, :]], axis=0
                )
                shapValue_list.extend(i for i in range(explain_sample))
                shapValue_name.extend(
                    "shap_样本_" + str(i) for i in range(explain_sample)
                )
        elif shapSet == 0: # 在全部数据中选择 SHAP 样本
            df_shapValue = pd.concat([Xtrain, Xtest], axis=0)
            df_shapValue_Y = pd.concat([Ytrain, Ytest], axis=0)
            if explain_sample == 4:
                flage1, flage2, flage3, flage4 = True, True, True, True
                for i in range(len(df_shapValue_Y)):
                    if (
                        flage1
                        and f(df_shapValue.iloc[i : i + 1, :])[0] >= resThreshold
                        and df_shapValue_Y.iloc[i,] == 1
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为1实际值为1")
                        shapValue_list.append(i)
                        flage1 = False
                    elif (
                        flage2
                        and f(df_shapValue.iloc[i : i + 1, :])[0] >= resThreshold
                        and df_shapValue_Y.iloc[i,] == 0
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为1实际值为0")
                        shapValue_list.append(i)
                        flage2 = False
                    elif (
                        flage3
                        and f(df_shapValue.iloc[i : i + 1, :])[0] < resThreshold
                        and df_shapValue_Y.iloc[i,] == 1
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为0实际值为1")
                        shapValue_list.append(i)
                        flage3 = False
                    elif (
                        flage4
                        and f(df_shapValue.iloc[i : i + 1, :])[0] < resThreshold
                        and df_shapValue_Y.iloc[i,] == 0
                    ):
                        df_shapValue_show = pd.concat(
                            [df_shapValue_show, df_shapValue.iloc[i : i + 1, :]], axis=0
                        )
                        shapValue_name.append("shap_样本_预测值为0实际值为0")
                        shapValue_list.append(i)
                        flage4 = False

                    if not flage1 and not flage2 and not flage3 and not flage4:
                        break
            else:
                df_shapValue_show = pd.concat(
                    [df_shapValue_show, df_shapValue.iloc[0:explain_sample, :]], axis=0
                )
                shapValue_list.extend(i for i in range(explain_sample))
                shapValue_name.extend(
                    "shap_样本_" + str(i) for i in range(explain_sample)
                )
        explainer = shap.KernelExplainer(f, med)
        shap_values = explainer.shap_values(df_shapValue)

        if explain_numvar > 0:
            # SHAP beeswarm summary plot
            assert explain_numvar <= len(features)
            plt.clf()
            plt.ioff()
            shap.summary_plot(shap_values, df_shapValue, show=False, max_display=50) # 绘制 SHAP beeswarm 图
            fig = plt.gcf()

            if savePath is not None:
                plot_name_dict["SHAP_变量贡献度总结"] = save_fig( # 保存 SHAP beeswarm 图
                    savePath, "shap_summary", "png", fig, str_time=str_time
                )
                plot_name_dict_save["SHAP_变量贡献度总结"] = save_fig( # 保存指定格式 SHAP beeswarm 图
                    savePath, "shap_summary", picFormat, fig, str_time=str_time
                )
            plt.close(fig)

            shap.summary_plot(
                shap_values, df_shapValue, plot_type="bar", show=False,max_display=50
            )
            fig1 = plt.gcf()

            if savePath is not None:
                plot_name_dict["SHAP_重要性图"] = save_fig(
                    savePath, "shap_import", "png", fig1, str_time=str_time
                )
                plot_name_dict_save["SHAP_重要性图"] = save_fig(
                    savePath, "shap_import", picFormat, fig1, str_time=str_time
                )
            plt.close(fig1)
            if shap_catter and len(shap_catter_feas) > 0:
                for fea in shap_catter_feas:
                    shap.dependence_plot(
                        fea,
                        shap_values,
                        df_shapValue,
                        interaction_index=None,
                        show=False,
                    )
                    fig2 = plt.gcf()
                    if savePath is not None:
                        plot_name_dict["SHAP_点图_" + fea] = save_fig(
                            savePath,
                            "shap_catter_" + fea,
                            "png",
                            fig2,
                            str_time=str_time,
                        )
                        plot_name_dict_save["SHAP_点图_" + fea] = save_fig(
                            savePath,
                            "shap_catter_" + fea,
                            picFormat,
                            fig2,
                            str_time=str_time,
                        )
                        plt.close(fig2)


        if explain_sample > 0:
            for i in range(len(shapValue_list)):
                # SHAP explain
                shap.force_plot( # 绘制 SHAP force plot
                    explainer.expected_value,
                    shap_values[shapValue_list[i]],
                    df_shapValue_show.iloc[i, :],
                    show=False,
                    figsize=(15, 3),
                    matplotlib=True,
                )
                fig = plt.gcf()
                if savePath is not None:
                    plot_name_dict[shapValue_name[i]] = save_fig(
                        savePath,
                        "shap_sample_{}".format(i + 1),
                        "png",
                        fig,
                        str_time=str_time,
                    )
                    plot_name_dict_save[shapValue_name[i]] = save_fig(
                        savePath,
                        "shap_sample_{}".format(i + 1),
                        picFormat,
                        fig,
                        str_time=str_time,
                    )
                plt.close(fig)

                if shap_waterfall:
                    sample_idx = shapValue_list[i]
                    sample_shap_values = shap_values[sample_idx]
                    sample_features = df_shapValue_show.iloc[i, :]
                    explanation_object = shap.Explanation(
                        values=sample_shap_values,
                        base_values=explainer.expected_value,  # 模型的基线值
                        data=sample_features.values,  # 样本特征值
                        feature_names=df_shapValue_show.columns.tolist()  # 特征名
                    )

                    shap.waterfall_plot(explanation_object, max_display=50, show=False)

                    fig_waterfall = plt.gcf()

                    if savePath is not None:
                        waterfall_name = shapValue_name[i].replace("shap_样本", "shap_waterfall_樣本")
                        plot_name_dict[waterfall_name] = save_fig(
                            savePath,
                            "shap_waterfall_{}".format(i + 1),
                            "png",
                            fig_waterfall,
                            str_time=str_time,
                        )
                        plot_name_dict_save[waterfall_name] = save_fig(
                            savePath,
                            "shap_waterfall_{}".format(i + 1),
                            picFormat,
                            fig_waterfall,
                            str_time=str_time,
                        )
                    plt.close(fig_waterfall)


    # 将寻参相关的图片文件名字典更新到主图片字典
    plot_name_dict.update(plot_name_dict1)

    #总时间汇总
    time_summary_str = "\n\n--- 流程执行时间汇总 ---\n"
    for step_name, duration in timing_results.items():
        time_summary_str += f"{step_name}: {duration:.4f} 秒\n"

    #替换计时信息
    str_result = str_result.replace("[TIMING_PLACEHOLDER]", time_summary_str)

    result_dict = {
        "str_result": {"分析结果描述": str_result},
        "tables": df_dict,
        "pics": plot_name_dict,
        "save_pics": plot_name_dict_save,
        "model": result_model_save
    }
    return result_dict


9.多模型比较
import os
import random
import datetime
import pickle
import math
import pandas as pd
import numpy as np
import matplotlib
# import joblib
# from skopt import BayesSearchCV
# from statsmodels.formula.api import ols
current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# from AnalysisFunction.utils_ml.params import BayesDefaultRange

matplotlib.use("AGG")
import matplotlib.pyplot as plt

from sklearn.base import clone
# from sklearn.preprocessing import StandardScaler, OrdinalEncoder
# from sklearn.inspection import permutation_importance
# from skopt import BayesSearchCV
# from sklearn.model_selection import cross_val_predict
# from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split as TTS
# from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold
# from sklearn.model_selection import ShuffleSplit
# from sklearn.model_selection import RandomizedSearchCV

# from xgboost import XGBRegressor
# from sklearn.linear_model import LassoCV, RidgeCV
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
# from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.svm import LinearSVR
# from sklearn.neighbors import KNeighborsRegressor
#
# from sklearn.mixture import GaussianMixture
# from sklearn.cluster import Birch
# from sklearn.cluster import KMeans
# from sklearn.cluster import AffinityPropagation
# from sklearn.cluster import SpectralClustering
# from sklearn.cluster import AgglomerativeClustering
#
# from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
# from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
# from sklearn.ensemble import BaggingClassifier
# from sklearn.ensemble import ExtraTreesClassifier
# from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
# from sklearn.naive_bayes import GaussianNB
# from sklearn.naive_bayes import ComplementNB
from sklearn.svm import SVC
#
# from sklearn.feature_selection import RFECV
# from sklearn.decomposition import PCA
# from AnalysisFunction.utils_ml.FeatrueSelect import mrmr_classif
# from AnalysisFunction.utils_ml.FeatrueSelect import ReliefF
#
# from sklearn.metrics import precision_recall_curve
# from sklearn.metrics import average_precision_score
from sklearn.metrics import auc
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    mutual_info_score,
    v_measure_score,
    normalized_mutual_info_score,
    silhouette_samples,
)

from sklearn.metrics import brier_score_loss
from sklearn.calibration import CalibratedClassifierCV, calibration_curve

from sklearn.preprocessing import label_binarize

# from xgboost import plot_importance
import AnalysisFunction.X_5_SmartPlot as x5
from AnalysisFunction.X_5_SmartPlot import plot_calibration_curve
from AnalysisFunction.X_5_SmartPlot import calculate_net_benefit
from AnalysisFunction.X_5_SmartPlot import plot_decision_curves
from AnalysisFunction.X_1_DataGovernance import data_standardization
from AnalysisFunction.X_1_DataGovernance import _analysis_dict
from AnalysisFunction.X_2_DataSmartStatistics import comprehensive_smart_analysis

from AnalysisFunction.utils_ml import filtering, dic2str, round_dec, save_fig
from AnalysisFunction.utils_ml import (
    classification_metric_evaluate,
    regression_metric_evaluate,
)
from AnalysisFunction.utils_ml import (
    make_class_metrics_dict,
    make_regr_metrics_dict,
    multiclass_metric_evaluate,
)
from AnalysisFunction.utils_ml import ci

# from AnalysisFunction.utils_ml import (
#     GridSearcherCV,
#     RandSearcherCV,
#     GridSearcherSelf,
#     RandSearcherSelf,
# )
from AnalysisFunction.utils_ml.params import GridDefaultRange, BayesDefaultRange
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV##,HalvingGridSearchCV
from AnalysisFunction.utils_ml.params import RandDefaultRange
from AnalysisFunction.utils_ml.auc_delong import delong_roc_test

from functools import reduce
from yellowbrick.cluster import SilhouetteVisualizer
from yellowbrick.cluster import KElbowVisualizer
from yellowbrick.utils import check_fitted
from yellowbrick.classifier import ConfusionMatrix
import matplotlib as mpl
mpl.rcParams['axes.grid'] = False


plt.rcParams["font.sans-serif"] = ['Times New Roman + SimSun']  # 用来正常显示中文标签
plt.rcParams["axes.unicode_minus"] = False  # 用来正常显示负号)

plt.rcParams["ps.useafm"] = True
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams["pdf.fonttype"] = 42

random_model = ['LGBMClassifier', 'XGBClassifier', 'XGBRegressor', 'RandomForestClassifier', 'AdaBoostClassifier','DecisionTreeClassifier'
              'MLPClassifier', 'SVC', 'LogisticRegression', 'LogisticRegressionCV', 'RandomForestRegressor', 'GradientBoostingClassifier'
              'AdaBoostRegressor', 'LinearSVR', 'LassoCV', 'PCA', 'GaussianMixture', 'KMeans', 'SpectralClustering']

def two_groups_classfication_multimodels(
    df_input,
    group,
    features,
    methods=[],
    decimal_num=3,
    testsize=0.2,
    boostrap=5,
    randomState=42,
    smooth=False,
    searching="default",
    label='LABEL',
    trainSet=False,
    trainLabel=0,
    PR=False,
    style='lancet',
    dpi=600,
    picFormat="jpeg",
    isKFold="cross",
    savePath=None,
    resultType=0,
    delong=False,
    DCA_cut=0,
    **kwargs,
):
    """
    df_input:Dataframe
    features:自变量list
    group：因变量str
    testsize: 测试集比例
    boostrap：重采样次数
    searching:bool 是否进行自动寻参，默认为否
    savePath:str 图片存储路径
    ##交叉验证 cross ,重采样 resample,嵌套交叉验证nest
    """
    palette_dict = {
        'lancet': ["#00468BFF", "#ED0000FF", "#42B540FF", "#0099B4FF", "#925E9FFF", "#FDAF91FF", "#AD002AFF",
                   "#ADB6B6FF",
                   "#1B1919FF"],
        'nejm': ["#BC3C29FF", "#0072B5FF", "#E18727FF", "#20854EFF", "#7876B1FF", "#6F99ADFF", "#FFDC91FF", "#EE4C97FF",
                 "#BC3C29FF"],
        'jama': ["#374E55FF", "#DF8F44FF", "#00A1D5FF", "#B24745FF", "#79AF97FF", "#6A6599FF", "#80796BFF", "#374E55FF",
                 "#DF8F44FF"],
        'npg': ["#E64B35FF", "#4DBBD5FF", "#00A087FF", "#3C5488FF", "#F39B7FFF", "#8491B4FF", "#91D1C2FF", "#DC0000FF",
                "#7E6148FF", "#B09C85FF"]}
    str_time = (
        str(datetime.datetime.now().hour)
        + str(datetime.datetime.now().minute)
        + str(datetime.datetime.now().second)
    )
    random_number = random.randint(1, 100)
    str_time = str_time + str(random_number)

    features_flag = False
    if boostrap == 1 and trainSet:
        if label in features or label == group:
            return {"error": "标签列不能在所在模型中，请重新选择数据划分标签列！"+"false-error"}
        dftemp = df_input[features + [group]+[label]].dropna()
    else:
        dftemp = df_input[features + [group]].dropna()
    x = dftemp[features]
    y = dftemp[[group]]



    u = np.sort(np.unique(np.array(dftemp[group])))
    if len(u) == 2 and set(u) != set([0, 1]):
        y_result = label_binarize(dftemp[group], classes=[ii for ii in u])  # 将标签二值化
        y_result_pd = pd.DataFrame(y_result, columns=[group])
        df = pd.concat([dftemp.drop(group, axis=1), y_result_pd], axis=1)
        x = df[features]
        y = df[[group]]
    elif len(u) > 2:
        return {"error": "暂时只支持二分类。请检查因变量取值情况。"+"false-error"}

    name_dict = {
        "LogisticRegression": "logistic",
        "XGBClassifier": "XGBoost",
        "RandomForestClassifier": "RandomForest",
        "LGBMClassifier": "LightGBM",
        "SVC": "SVM",
        "MLPClassifier": "MLP",
        "GaussianNB": "GNB",
        "ComplementNB": "CNB",
        "AdaBoostClassifier": "AdaBoost",
        "KNeighborsClassifier": "KNN",
        "DecisionTreeClassifier": "DecisionTree",
        "BaggingClassifier": "Bagging",
        "GradientBoostingClassifier": 'GBDT',
    }
    if len(methods) == 0:
        methods = [
            "LogisticRegression",
            "XGBClassifier",
            "RandomForestClassifier",
            # 'SVC',
            # 'MLPClassifier',
            # 'AdaBoostClassifier',
            # 'KNeighborsClassifier',
            # 'DecisionTreeClassifier',
            # 'BaggingClassifier',
        ]
    str_result = "已采用多种机器学习模型尝试完成数据样本分类任务，包括：{}。各模型的参数值选取情况如下所示：\n\n".format(methods)

    plot_name_list = []
    plot_name_dict = {}
    plot_name_dict_save = {}

    fig, ax = plt.subplots(figsize=(4, 4), dpi=dpi)

    # 画对角线
    ax.plot(
        [0, 1],
        [0, 1],
        linestyle="--",
        lw=1,
        color="r",
        alpha=1.0,
    )
    ax.grid(which="major", axis="both", linestyle="-.", alpha=0.3, color="grey")
    ax.set_xlim([-0.02, 1.02])
    ax.set_ylim([-0.02, 1.02])

    ax.tick_params(top=False, right=False)
    # ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)
    ax.set_xlabel("1-Specificity")
    ax.set_ylabel("Sensitivity")
    ax.set_title("ROC curve(Validation)")

    mean_fpr = np.linspace(0, 1, 100)
    colors = x5.CB91_Grad_BP

    df_0 = pd.DataFrame(columns=list(make_class_metrics_dict().keys()), index=[0])
    df_0_test = df_0.copy()

    df_plot = pd.DataFrame(columns=["method", "mean", "std"])



    fpr_train_alls, tpr_train_alls, train_method_alls, mean_auc_train_alls = (
        [],
        [],
        [],
        [],
    )
    fraction_of_positives_alls, mean_predicted_value_alls, clf_score_alls = [], [], []
    AUC_95CI_test, AUC_95CI_SD_test, AUC_95CI_train, AUC_95CI_SD_train = [], [], [], []
    brier_scores_all = []

    DCA_dict = {}
    model_test_data_all = {}
    sdorci = " SD " if resultType == 0 else " 95%CI "

    X_train_ps, Y_train_ps, model_train_s = [], [], []  ###PR曲线
    X_test_ps, Y_test_ps = [], []
    name = []

    for i, method in enumerate(methods):
        tprs_train, tprs_test = [], []

        name.append(name_dict[method])
        if searching == "auto":
            # if method == "LGBMClassifier":
            #     searcher = GridSearcherCV("Classification", globals()[method]())
            #     selected_model = searcher(x, y)
            # else:
            #     searcher = RandSearcherCV("Classification", globals()[method]())
            #     selected_model = searcher(x, y)  # ; searcher.report()
            searcher = RandomizedSearchCV(globals()[method](), param_distributions=GridDefaultRange[method])
            # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
            selected_model = searcher.fit(x, y)
        elif searching == "default":
            # selected_model = globals()[method]() if (method != 'SVC') else globals()[method](probability=True)
            if method == "SVC":
                selected_model = globals()[method](probability=True,random_state=42)
            elif method == "MLPClassifier":
                selected_model = globals()[method](
                    hidden_layer_sizes=(20, 10), max_iter=20,random_state=42
                )
            elif method == "RandomForestClassifier":
                selected_model = globals()[method](n_estimators=20,random_state=42)
            else:
                if method in random_model:
                    selected_model = globals()[method](random_state=42)
                else:
                    selected_model = globals()[method]()


        elif searching == "handle":
            method_dicts = kwargs
            if i == 0:
                me_count = True
                for me_list in methods:
                    if me_list in method_dicts.keys():
                        me_count = False
                        continue
                if me_count:
                    return {"error": "请设置要调参的模型！"+"false-error"}
            if method in method_dicts.keys():
                method_dict = {}
                if method == "SVC":
                    method_dict.update({"probability": True})
                method_dict.update(method_dicts[method])
                if (
                    method == "RandomForestClassifier"
                    and method_dict["max_depth"] == "None"
                ):
                    method_dict["max_depth"] = None
                if (
                    method == "DecisionTreeClassifier"
                    and method_dict["max_depth"] == "None"
                ):
                    method_dict["max_depth"] = None
                if method == "MLPClassifier":
                    hls_vals = str(method_dict["hidden_layer_sizes"]).split(",")
                    hls_value = ()
                    for hls_val in hls_vals:
                        try:
                            if int(hls_val) >= 5 and int(hls_val) <= 200:
                                hls_value = hls_value + (int(hls_val),)
                            else:
                                return {"error": "请按照要求重新设置隐藏层宽度！"+"false-error"}
                        except:
                            return {"error": "请重新设神经网络模型中的隐藏层宽度！"+"false-error"}
                    method_dict["hidden_layer_sizes"] = hls_value
                if method == "GaussianNB" and method_dict["priors"] == "None":
                    method_dict["priors"] = None
                elif method == "GaussianNB":
                    pri_vals = str(method_dict["priors"]).split(",")
                    pri_value = ()
                    pri_sum = 0.0
                    for pri_val in pri_vals:
                        try:
                            pri_sum = float(pri_val) + pri_sum
                            pri_value = pri_value + (float(pri_val),)
                        except:
                            return {"error": "请重新设朴素贝叶斯模型中的先验概率！"+"false-error"}
                    if len(pri_vals) == len(np.unique(y)) and pri_sum == 1.0:
                        method_dict["priors"] = pri_value
                    else:
                        return {"error": "请重新设朴素贝叶斯模型中的先验概率！"+"false-error"}
                if method in random_model:
                    method_dict["random_state"] = 42
                selected_model = globals()[method](**method_dict)
            else:
                # if method == "LGBMClassifier":
                #     searcher = GridSearcherCV("Classification", globals()[method]())
                #     selected_model = searcher(x, y)
                # else:
                #     searcher = RandSearcherCV("Classification", globals()[method]())
                #     selected_model = searcher(x, y)  # ; searcher.report()
                searcher = RandomizedSearchCV(globals()[method](), param_distributions=GridDefaultRange[method])
                # searcher=GridSearchCV(globals()[method](),param_grid=GridDefaultRange[method])
                selected_model = searcher.fit(x, y)
                selected_model = globals()[method](**selected_model.best_params_)

        list_evaluate_dic_train = make_class_metrics_dict()
        list_evaluate_dic_test = make_class_metrics_dict()

        clf_score = 1
        fraction_of_positives = np.array([1])
        mean_predicted_value = np.array([1])

        p_serie_s_te, net_benefit_serie_s_te, net_benefit_serie_All_s_te = [], [], []

        data_all = {}
        test_data_delong = {}
        conf_dic_train, conf_dic_test = {}, {}
        if isKFold == "cross":
            # KF = KFold(n_splits=boostrap, random_state=42,shuffle=True)
            KF = StratifiedKFold(
                n_splits=boostrap, random_state=randomState, shuffle=True
            )
            for i_k, (train_index, valid_index) in enumerate(KF.split(x, y)):
                # 划分训练集和验证集
                Xtrain, Xtest = x.iloc[train_index], x.iloc[valid_index]
                Ytrain, Ytest = y.iloc[train_index], y.iloc[valid_index]
                data_all.update(
                    {
                        i_k: {
                            "Xtrain": Xtrain,
                            "Ytrain": Ytrain,
                            "Xtest": Xtest,
                            "Ytest": Ytest,
                        }
                    }
                )
                test_data_delong.update({i_k: np.array(Ytest).T[0]})
        elif isKFold == "nest":
            KF = StratifiedKFold(
                n_splits=boostrap, random_state=randomState, shuffle=True
            )
            inner_cv = StratifiedKFold(
                n_splits=boostrap, random_state=randomState, shuffle=True
            )
            for i_k, (train_index, valid_index) in enumerate(KF.split(x, y)):
                # 划分训练集和验证集
                Xtrain, Xtest = x.iloc[train_index], x.iloc[valid_index]
                Ytrain, Ytest = y.iloc[train_index], y.iloc[valid_index]
                data_all.update(
                    {
                        i_k: {
                            "Xtrain": Xtrain,
                            "Ytrain": Ytrain,
                            "Xtest": Xtest,
                            "Ytest": Ytest,
                        }
                    }
                )
                test_data_delong.update({i_k: np.array(Ytest).T[0]})
        else:
            for index in range(0, boostrap):
                if boostrap == 1:
                    if trainSet:
                        if isinstance(dftemp[label][0], str):
                            trainLabel = str(trainLabel)
                        train_a  = dftemp[dftemp[label] == trainLabel]
                        test_a = dftemp[dftemp[label] != trainLabel]
                        train_all = train_a.drop(label, axis=1)
                        test_all = test_a.drop(label, axis=1)
                        # dftemp = dftemp.drop(label, axis=1)
                        Xtrain = train_all.drop(group, axis=1)
                        Ytrain = train_all.loc[:, [group]].squeeze(axis=1)
                        Xtest = test_all.drop(group, axis=1)
                        Ytest = test_all.loc[:, [group]].squeeze(axis=1)
                    else:
                        X = dftemp.drop(group, axis=1)
                        Y = dftemp.loc[:, [group]].squeeze(axis=1)
                        Xtrain, Xtest, Ytrain, Ytest = TTS(X, Y, test_size=testsize, random_state=randomState, )
                else:
                    if searching == "handle":
                        Xtrain, Xtest, Ytrain, Ytest = TTS(
                            x, y, test_size=testsize, random_state=index
                        )
                    else:
                        Xtrain, Xtest, Ytrain, Ytest = TTS(x, y, test_size=testsize)
                data_all.update(
                    {
                        index: {
                            "Xtrain": Xtrain,
                            "Ytrain": Ytrain,
                            "Xtest": Xtest,
                            "Ytest": Ytest,
                        }
                    }
                )
                test_data_delong.update({index: np.array(Ytest).T[0]})
        if method == methods[0]:
            model_test_data_all.update({"original": test_data_delong})
        # for index in range(0, boostrap):
        test_all_data_delong = {}
        X_train_p, Y_train_p, model_train = [], [], []  ##PR曲线
        X_test_p, Y_test_p = [], []
        brier_scores=[]
        for data_key, data_value in data_all.items():
            if isKFold == "nest":
                best_auc = 0.0
                resThreshold = 0
                Xtrain, Ytrain, Xtest, Ytest = (
                    data_value["Xtrain"],
                    data_value["Ytrain"],
                    data_value["Xtest"],
                    data_value["Ytest"],
                )
                for j, (train_index_inner, test_index_inner) in enumerate(
                    inner_cv.split(Xtrain, Ytrain)
                ):
                    X_train_inner, X_test_inner = (
                        Xtrain.iloc[train_index_inner],
                        Xtrain.iloc[test_index_inner],
                    )
                    y_train_inner, y_test_inner = (
                        Ytrain.iloc[train_index_inner],
                        Ytrain.iloc[test_index_inner],
                    )

                    model_i = clone(selected_model).fit(X_train_inner, y_train_inner)
                    # 利用classification_metric_evaluate函数获取在验证集的预测值
                    _, _, metric_dic_train_i, _ = classification_metric_evaluate(
                        model_i, X_train_inner, y_train_inner, True
                    )
                    _, _, metric_dic_valid_i, _ = classification_metric_evaluate(
                        model_i,
                        X_test_inner,
                        y_test_inner,
                        True,
                        Threshold=metric_dic_train_i["cutoff"],
                    )
                    # metric_dic_valid.update({'cutoff': metric_dic_train_i['cutoff']})
                    if metric_dic_valid_i["AUC"] > best_auc:
                        model = model_i
                        resThreshold = metric_dic_train_i["cutoff"]
            else:
                Xtrain, Ytrain, Xtest, Ytest = (
                    data_value["Xtrain"],
                    data_value["Ytrain"],
                    data_value["Xtest"],
                    data_value["Ytest"],
                )

                model = clone(selected_model).fit(Xtrain, Ytrain)
            ####################################
            X_train_p.append(Xtrain)
            Y_train_p.append(Ytrain)
            model_train.append(model)
            X_test_p.append(Xtest)
            Y_test_p.append(Ytest)
            ##########################################
            Yprob = model.predict_proba(Xtest)[:, 1]
            test_all_data_delong.update({data_key: Yprob})
            (
                prob_pos,
                p_serie,
                net_benefit_serie,
                net_benefit_serie_All,
            ) = calculate_net_benefit(model, Xtest, Ytest)
            p_serie_s_te.append(p_serie)
            net_benefit_serie_s_te.append(net_benefit_serie)
            net_benefit_serie_All_s_te.append(net_benefit_serie_All)
            """
            if hasattr(model, "predict_proba"):
                prob_pos = model.predict_proba(Xtest)[:, 1]
            else:  # use decision function
                prob_pos = model.decision_function(Xtest)
                prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())
            """
            clf_score1 = brier_score_loss(
                Ytest, prob_pos, pos_label=y[group].max()
            )  ##strategy='quantile',
            brier_scores.append(clf_score1)
            if clf_score > clf_score1:
                clf_score = clf_score1
                fraction_of_positives, mean_predicted_value = calibration_curve(
                    Ytest, prob_pos, n_bins=10
                )

            # 利用classification_metric_evaluate函数获取在测试集的预测值
            try:
                if isKFold == "nest":
                    (
                        fpr_train,
                        tpr_train,
                        metric_dic_train,
                        _,
                    ) = classification_metric_evaluate(
                        model, Xtrain, Ytrain, Threshold=resThreshold
                    )
                else:
                    (
                        fpr_train,
                        tpr_train,
                        metric_dic_train,
                        _,
                    ) = classification_metric_evaluate(model, Xtrain, Ytrain)

                fpr_test, tpr_test, metric_dic_test, _ = classification_metric_evaluate(
                    model, Xtest, Ytest, Threshold=metric_dic_train["cutoff"]
                )
                metric_dic_test.update({"cutoff": metric_dic_train["cutoff"]})
            except Exception as e:
                return {
                    "error": "数据不均衡，至少有一组验证集中存在结局全部为0或者1的数据！请选择另外一种方法重采样（交叉验证）的方法处理！"
                }

            # interp:插值 把结果添加到tprs列表中
            tprs_train.append(np.interp(mean_fpr, fpr_train, tpr_train))
            tprs_test.append(np.interp(mean_fpr, fpr_test, tpr_test))
            tprs_train[-1][0] = 0.0
            tprs_test[-1][0] = 0.0

            # 计算所有评价指标
            for key in list_evaluate_dic_train.keys():
                list_evaluate_dic_train[key].append(metric_dic_train[key])
                list_evaluate_dic_test[key].append(metric_dic_test[key])

        model_test_data_all.update({method: test_all_data_delong})
        DCA_dict[name_dict[method]] = {
            "p_serie": p_serie_s_te,
            "net_b_s": net_benefit_serie_s_te,
            "net_b_s_A": net_benefit_serie_All_s_te,
        }

        X_train_ps.append(X_train_p)
        Y_train_ps.append(Y_train_p)
        model_train_s.append(model_train)

        X_test_ps.append(X_test_p)
        Y_test_ps.append(Y_test_p)
        ###画校准曲线
        # X_train, X_test, Y_train, Y_test = TTS(x, y, test_size=testsize, random_state=0)
        # model_CC = clone(selected_model).fit(X_train, Y_train)
        # y_pred = model.predict(Xtest)
        # if hasattr(model_CC, "predict_proba"):
        #    prob_pos = model_CC.predict_proba(X_test)[:, 1]
        # else:  # use decision function
        #    prob_pos = model_CC.decision_function(X_test)
        #    prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())

        # clf_score = brier_score_loss(Y_test, prob_pos, pos_label=y.max())##strategy='quantile',
        # fraction_of_positives, mean_predicted_value = calibration_curve(Y_test, prob_pos,
        #                                                                        n_bins=10)
        clf_score_alls.append(clf_score)
        brier_scores_all.append(brier_scores)
        fraction_of_positives_alls.append(fraction_of_positives)
        mean_predicted_value_alls.append(mean_predicted_value)

        for key in list_evaluate_dic_train.keys():
            metric_dic_train[key] = np.mean(list_evaluate_dic_train[key])
            metric_dic_test[key] = np.mean(list_evaluate_dic_test[key])

            if resultType == 0:  ##SD
                list_evaluate_dic_train[key] = np.std(
                    list_evaluate_dic_train[key], axis=0
                )
                list_evaluate_dic_test[key] = np.std(
                    list_evaluate_dic_test[key], axis=0
                )
            elif resultType == 1:  ##CI
                conf_dic_train[key] = list(ci(list_evaluate_dic_train[key]))
                conf_dic_test[key] = list(ci(list_evaluate_dic_test[key]))
        result_dic_train = metric_dic_train
        result_dic_test = metric_dic_test
        if resultType == 0:  ##SD
            for tem in ["AUC_L", "AUC_U"]:
                del list_evaluate_dic_train[tem]
                del list_evaluate_dic_test[tem]
            for key in list_evaluate_dic_train.keys():
                if key == "AUC":
                    result_dic_train["AUC(95%CI)"] = (
                        str(round_dec(float(metric_dic_train[key]), d=decimal_num))
                        + "("
                        + str(
                            round_dec(
                                float(list_evaluate_dic_train[key]), d=decimal_num
                            )
                        )
                        + ")"
                    )

                    result_dic_test["AUC(95%CI)"] = (
                        str(round_dec(float(metric_dic_test[key]), d=decimal_num))
                        + "("
                        + str(
                            round_dec(float(list_evaluate_dic_test[key]), d=decimal_num)
                        )
                        + ")"
                    )
                else:
                    result_dic_train[key] = (
                        str(round_dec(float(metric_dic_train[key]), d=decimal_num))
                        + "("
                        + str(
                            round_dec(
                                float(list_evaluate_dic_train[key]), d=decimal_num
                            )
                        )
                        + ")"
                    )
                    result_dic_test[key] = (
                        str(round_dec(float(metric_dic_test[key]), d=decimal_num))
                        + "("
                        + str(
                            round_dec(float(list_evaluate_dic_test[key]), d=decimal_num)
                        )
                        + ")"
                    )
        elif resultType == 1:
            for tem in ["AUC_L", "AUC_U"]:
                del conf_dic_train[tem]
                del conf_dic_test[tem]
            for key in conf_dic_train.keys():
                if key == "AUC":
                    result_dic_train["AUC(95%CI)"] = (
                        str(round_dec(float(metric_dic_train[key]), decimal_num))
                        + " ("
                        + str(round_dec(float(metric_dic_train["AUC_L"]), decimal_num))
                        + "-"
                        + str(round_dec(float(metric_dic_train["AUC_U"]), decimal_num))
                        + ")"
                    )

                    result_dic_test["AUC(95%CI)"] = (
                        str(round_dec(float(metric_dic_test[key]), decimal_num))
                        + " ("
                        + str(round_dec(float(metric_dic_test["AUC_L"]), decimal_num))
                        + "-"
                        + str(round_dec(float(metric_dic_test["AUC_U"]), decimal_num))
                        + ")"
                    )
                else:
                    result_dic_train[key] = (
                        str(round(float(metric_dic_train[key]), decimal_num))
                        + "("
                        + str(round_dec(float(conf_dic_train[key][0]), d=decimal_num))
                        + "-"
                        + str(round_dec(float(conf_dic_train[key][1]), d=decimal_num))
                        + ")"
                    )
                    result_dic_test[key] = (
                        str(round(float(metric_dic_test[key]), decimal_num))
                        + "("
                        + str(round_dec(float(conf_dic_test[key][0]), d=decimal_num))
                        + "-"
                        + str(round_dec(float(conf_dic_test[key][1]), d=decimal_num))
                        + ")"
                    )
        df_train_result = pd.DataFrame([result_dic_train], index=["Mean"])
        df_test_result = pd.DataFrame([result_dic_test], index=["Mean"])
        df_train_result["分类模型"] = name_dict[method]
        df_test_result["分类模型"] = name_dict[method]

        AUC_95CI_test.append(list(df_test_result.iloc[0, -4:-2]))
        AUC_95CI_train.append(list(df_train_result.iloc[0, -4:-2]))

        df_0 = pd.concat([df_0, df_train_result])
        df_0_test = pd.concat([df_0_test, df_test_result])

        mean_tpr_train = np.mean(tprs_train, axis=0)
        mean_tpr_test = np.mean(tprs_test, axis=0)
        mean_tpr_train[-1] = 1.0
        mean_tpr_test[-1] = 1.0
        mean_auc_train = auc(mean_fpr, mean_tpr_train)  # 计算训练集平均AUC值
        mean_auc_test = auc(mean_fpr, mean_tpr_test)

        ###画训练集ROC
        fpr_train_alls.append(mean_fpr)
        tpr_train_alls.append(mean_tpr_train)
        train_method_alls.append(method)
        mean_auc_train_alls.append(mean_auc_train)

        std_value = 0
        if resultType == 0:
            std_value = list_evaluate_dic_test["AUC"]
        elif resultType == 1:
            std_value = (conf_dic_train["AUC"][1] - conf_dic_train["AUC"][0]) / 2
        df_plot = df_plot.append(
            {
                "method": name_dict[method],
                "mean": mean_auc_test,
                "std": std_value,
            },
            ignore_index=True,
        )
        print(df_plot)
        ax.plot(
            mean_fpr,
            mean_tpr_test,
            c=palette_dict[style][i],
            label=name_dict[method]
            + "(AUC = "
            + df_test_result['AUC(95%CI)'][0][:df_test_result['AUC(95%CI)'][0].find('(')].strip()
            + sdorci
            + df_test_result['AUC(95%CI)'][0][df_test_result['AUC(95%CI)'][0].find('('):].strip()
            + ")",
            lw=1.5,
            alpha=1,
        )
        if searching == 'auto':
            str_result += (
                    method
                    + ": AUC="
                    + str(round_dec(mean_auc_train, decimal_num))
                    + ";  模型参数:\n"
                    + dic2str(selected_model.best_params_, method)
                    + "\n"
            )
        else:
            str_result += (
                    method
                    + ": AUC="
                    + str(round_dec(mean_auc_train, decimal_num))
                    + ";  模型参数:\n"
                    + dic2str(selected_model.get_params(), method)
                    + "\n"
            )
    ###模型德龙检测
    if delong:
        delong_z, delong_p = [], []
        for i in range(boostrap):
            zzz, ppp = [], []
            for method1 in methods:
                zz, pp = [], []
                for method2 in methods:
                    z, p = delong_roc_test(
                        model_test_data_all["original"][i],
                        model_test_data_all[method1][i],
                        model_test_data_all[method2][i],
                    )
                    zz.append(z[0][0])
                    pp.append(p[0][0])
                zzz.append(zz)
                ppp.append(pp)
            delong_z.append(zzz)
            delong_p.append(ppp)
        if boostrap == 1:
            delong_zz1 = pd.DataFrame(
                reduce(lambda x, y: np.array(x) + np.array(y), delong_z),
                index=methods,
                columns=methods,
            )
            delong_pp1 = pd.DataFrame(
                reduce(lambda x, y: np.array(x) + np.array(y), delong_p),
                index=methods,
                columns=methods,
            )
        else:
            delong_zz1 = pd.DataFrame(
                reduce(lambda x, y: np.array(x) + np.array(y), delong_z)
                / len(delong_z),
                index=methods,
                columns=methods,
            )
            delong_pp1 = pd.DataFrame(
                reduce(lambda x, y: np.array(x) + np.array(y), delong_p)
                / len(delong_p),
                index=methods,
                columns=methods,
            )

        delong_zz = delong_zz1.applymap(lambda x: round_dec(x, d=decimal_num))
        delong_pp = delong_pp1.applymap(lambda x: round_dec(x, d=decimal_num))
        delong_zz_name = pd.DataFrame(
            list(delong_zz.index), columns=["name"], index=list(delong_zz.index)
        )
        delong_zz = pd.concat([delong_zz_name, delong_zz], axis=1, ignore_index=False)
        delong_pp_name = pd.DataFrame(
            list(delong_pp.index), columns=["name"], index=list(delong_pp.index)
        )
        delong_pp = pd.concat([delong_pp_name, delong_pp], axis=1, ignore_index=False)

    # ymin = min([y - dy for y, dy in zip(df_plot['mean'], df_plot['std'])])
    # ymax = max([y + dy for y, dy in zip(df_plot['mean'], df_plot['std'])])
    # ymin, ymax = ymin - (ymax - ymin) / 4.0, ymax + (ymax - ymin) / 10.0

    if boostrap != 1:
        ymax = (
            np.max(df_plot["mean"])
            + np.max(df_plot["std"])
            + (np.max(df_plot["mean"]) - np.min(df_plot["mean"])) / 4
        )
        ymin = (
            np.min(df_plot["mean"])
            - np.max(df_plot["std"])
            - (np.max(df_plot["mean"]) - np.min(df_plot["mean"])) / 4
        )

        ymax = math.ceil(ymax * 100) / 100
        ymin = int(ymin * 100) / 100

    ax.legend(loc="lower right", fontsize=5)
    ax.legend(loc="lower right", fontsize=5)

    df_test_auc = []
    if savePath is not None:
        plot_name_list.append(
            save_fig(savePath, "valid_ROC_curve", "png", fig, str_time=str_time)
        )
        plot_name_dict_save["验证集ROC曲线"] = save_fig(
            savePath, "valid_ROC_curve", picFormat, fig, str_time=str_time
        )
        plt.close(fig)

        # 画训练集ROC
        fig1 = plt.figure(figsize=(4, 4), dpi=dpi)
        # 画对角线
        plt.plot(
            [0, 1],
            [0, 1],
            linestyle="--",
            lw=1,
            color="r",
            alpha=0.8,
        )
        plt.grid(which="major", axis="both", linestyle="-.", alpha=0.3, color="grey")

        for i in range(len(fpr_train_alls)):
            df_test_auc.append(df_0.iloc[i + 1]["AUC"])
            plt.plot(
                fpr_train_alls[i],
                tpr_train_alls[i],
                lw=1.5,
                alpha=0.9,
                c=palette_dict[style][i],
                label=name_dict[train_method_alls[i]]
                + "(AUC = "
                + df_0.iloc[i + 1]['AUC(95%CI)'][:df_0.iloc[i + 1]['AUC(95%CI)'].find('(')].strip()
                + sdorci
                + df_0.iloc[i + 1]['AUC(95%CI)'][df_0.iloc[i + 1]['AUC(95%CI)'].find('('):].strip()
                + ")",
            )

        plt.xlim([-0.02, 1.02])
        plt.ylim([-0.02, 1.02])
        plt.xlabel("1-Specificity")
        plt.ylabel("Sensitivity")
        plt.title("ROC curve(Training)")
        plt.legend(loc="lower right", fontsize=5)

        plot_name_list.append(
            save_fig(savePath, "ROC_Train_curve", "png", fig1, str_time=str_time)
        )
        plot_name_dict_save["训练集ROC曲线图"] = save_fig(
            savePath, "ROC_Train_curve", picFormat, fig1, str_time=str_time
        )
        plt.close(fig1)
        plot_name_list.reverse()  ###所有图片倒置

        if boostrap != 1:
            # df_plot.drop('mean', axis=1)
            # df_plot.loc[:,'mean']=pd.Series(df_test_auc,name='mean')
            plot_name_list += x5.forest_plot(
                df_input=df_plot,
                name="method",
                value="mean",
                err="std",
                direct="horizontal",
                fig_size=[len(methods) + 3, 9],
                ylim=[ymin, ymax],
                title="Forest Plot of Each Model AUC Score ",
                path=savePath,
                dpi=dpi,
                picFormat=picFormat,
            )
            plot_name_dict_save["验证集多模型森林图"] = plot_name_list[len(plot_name_list) - 1]
            plot_name_list.pop(len(plot_name_list) - 1)
    plt.close()
    ###画校准曲线
    if savePath is not None:
        from scipy.optimize import curve_fit
        from scipy.interpolate import make_interp_spline

        def fit_f(x, a, b):
            return a * np.arcsin(x) + b

        def fit_show(x, y_fit):
            a, b = y_fit.tolist()
            return a * np.arcsin(x) + b

        fig, ax1 = plt.subplots(figsize=(6, 6), dpi=dpi)
        ax1.plot([0, 1], [0, 1], "k:", label="Perfectly Calibrated")
        for i in range(len(mean_predicted_value_alls)):
            if resultType == 0:
                BS_SD = np.std(brier_scores_all[i])
                BS_score=" (%1.3f SD(%1.3f))" % (np.mean(brier_scores_all[i]),BS_SD)
            else:
                brier_down, brier_up = ci(brier_scores_all[i])
                BS_score=" (%1.3f 95%%CI(%1.3f-%1.3f))" % (
                np.mean(brier_scores_all[i]), brier_down, brier_up)
            if smooth and len(fraction_of_positives_alls[i]) >= 3:
                x_new = np.linspace(
                    min(mean_predicted_value_alls[i]),
                    max(mean_predicted_value_alls[i]),
                    len(fraction_of_positives_alls[i]) * 10,
                )
                try:
                    p_fit, _ = curve_fit(
                        fit_f,
                        mean_predicted_value_alls[i],
                        fraction_of_positives_alls[i],
                        maxfev=10000,
                    )
                    y_smooth = fit_show(x_new, p_fit)
                    # y_fit = np.polyfit(mean_predicted_value_alls[i], fraction_of_positives_alls[i], 3)
                    # y_smooth = f_fit(x_new, y_fit)
                    # y_smooth = spline(mean_predicted_value_alls[i], fraction_of_positives_alls[i], x_new)

                    ax1.plot(
                        x_new,
                        y_smooth,
                        c=palette_dict[style][i],
                        label=name_dict[methods[i]]+BS_score,
                    )
                except Exception as e:
                    ax1.plot(
                        mean_predicted_value_alls[i],
                        fraction_of_positives_alls[i],
                        "s-",
                        c=palette_dict[style][i],
                        label=name_dict[methods[i]]+BS_score,
                    )
            else:
                ax1.plot(
                    mean_predicted_value_alls[i],
                    fraction_of_positives_alls[i],
                    "s-",
                    c=palette_dict[style][i],
                    label=name_dict[methods[i]]+BS_score,
                )

        ax1.set_xlabel("Mean predicted value")
        ax1.set_ylabel("Fraction of positives")
        ax1.set_ylim([-0.05, 1.05])
        ax1.legend(loc="lower right")
        ax1.set_title("Calibration curve(Validation)")##Calibration plots  (reliability curve)
        plt.gca()
        plt.close()
        plot_name = "Calibration_curve_" + str_time
        plot_name_list.append(
            save_fig(savePath, plot_name, "png", fig, str_time=str_time)
        )
        plot_name_dict_save["验证集多模型校准曲线"] = save_fig(
            savePath, plot_name, picFormat, fig, str_time=str_time
        )

    ###画DCA曲线
    if savePath is not None:
        decision_curve_p = plot_decision_curves(
            DCA_dict,
            colors=palette_dict[style],
            name="Validation",
            savePath=savePath,
            dpi=dpi,
            picFormat=picFormat,
            DCA_cut=DCA_cut
        )
        plot_name_list.append(decision_curve_p[0])
        plot_name_dict_save["验证集DCA曲线图"] = decision_curve_p[1]

    df_train_result1 = df_0.drop([0])
    df_test_result1 = df_0_test.drop([0])

    classfier = df_train_result1.pop("分类模型")

    df_train_result = df_train_result1.applymap(lambda x: round_dec(x, d=decimal_num))
    df_train_result.insert(0, "分类模型", classfier)

    df_test_result1.pop("分类模型")
    df_test_result = df_test_result1.applymap(lambda x: round_dec(x, d=decimal_num))
    df_test_result.insert(0, "分类模型", classfier)

    AUC_95CI_tr = df_train_result.pop("AUC(95%CI)")
    df_train_result.insert(1, "AUC(95%CI)", AUC_95CI_tr)
    AUC_95CI_te = df_test_result.pop("AUC(95%CI)")
    df_test_result.insert(1, "AUC(95%CI)", AUC_95CI_te)

    df_train_result = df_train_result.drop(["AUC_L", "AUC_U"], axis=1)
    df_test_result = df_test_result.drop(["AUC_L", "AUC_U"], axis=1)

    if features_flag:
        df_count_r = round_dec(Xtest.shape[0] / x.shape[0], decimal_num)
    else:
        df_count_r = round_dec(testsize, decimal_num)
    if isKFold:
        str_result += (
            "\n下示森林图展示了各模型进行"
            + group
            + "预测的ROC结果,图中的误差线为ROC均值及SD。\n"
            + "模型的ROC均值及SD的是通过"
            + str(boostrap)
            + "折交叉验证,"
            + "模型中的变量包括"
            + ",".join(features)
            + "。\n"
        )
    else:
        str_result += (
            "\n下示森林图展示了各模型进行"
            + group
            + "预测的ROC结果,图中的误差线为ROC均值及SD。\n"
            + "模型的ROC均值及SD的是通过多次重复采样计算，重复采样次数为"
            + str(boostrap)
            + "次,"
            + "每一次重采样训练的验证集占总体样本的"
            + str(df_count_r * 100)
            + "%,训练集占"
            + str((1 - df_count_r) * 100)
            + "%,"
            + "模型中的变量包括"
            + ",".join(features)
            + "。\n"
        )

    best_ = (
        df_train_result.loc[df_train_result.index == "Mean"]
        .sort_values(by="AUC", ascending=False)
        .head(1)
    )
    name_train = best_.iloc[0]["分类模型"]
    str_result += "在目前所有模型中，训练集表现最佳者为{}（依据AUC排序），在各评价标准中其在训练集对应分数分别为：\n".format(
        name_train
    )
    for col in best_.columns[1:]:
        str_result += "\t{}：{}\n".format(col, best_.iloc[0][col])

    best_ = (
        df_test_result.loc[df_test_result.index == "Mean"]
        .sort_values(by="AUC", ascending=False)
        .head(1)
    )
    name_test = best_.iloc[0]["分类模型"]
    str_result += "验证集表现最佳者为{}（依据AUC排序），在各评价标准中其在验证集对应分数分别为：\n".format(name_test)
    for col in best_.columns[1:]:
        str_result += "\t{}：{}\n".format(col, best_.iloc[0][col])

    if name_test == name_train:
        str_result += "二者吻合，可以认为{}是针对此数据集的最佳模型选择。".format(name_train)
    else:
        str_result += "二者不吻合，{}极可能存在过拟合现象，{}可能稳定性相对较好。具体模型选择可根据下表详细评分信息进行取舍。".format(
            name_train, name_test
        )

    if resultType == 0:
        df_train_result.rename(
            columns={
                "AUC(95%CI)": "AUC(SD)",
                "cutoff": "cutoff(SD)",
                "准确度": "准确度(SD)",
                "灵敏度": "灵敏度(SD)",
                "特异度": "特异度(SD)",
                "阳性预测值": "阳性预测值(SD)",
                "阴性预测值": "阴性预测值(SD)",
                "F1分数": "F1分数(SD)",
                "Kappa": "Kappa(SD)",
            },
            inplace=True,
        )
        df_test_result.rename(
            columns={
                "AUC(95%CI)": "AUC(SD)",
                "cutoff": "cutoff(SD)",
                "准确度": "准确度(SD)",
                "灵敏度": "灵敏度(SD)",
                "特异度": "特异度(SD)",
                "阳性预测值": "阳性预测值(SD)",
                "阴性预测值": "阴性预测值(SD)",
                "F1分数": "F1分数(SD)",
                "Kappa": "Kappa(SD)",
            },
            inplace=True,
        )
    elif resultType == 1:
        df_train_result.rename(
            columns={
                "cutoff": "cutoff(95%CI)",
                "准确度": "准确度(95%CI)",
                "灵敏度": "灵敏度(95%CI)",
                "特异度": "特异度(95%CI)",
                "阳性预测值": "阳性预测值(95%CI)",
                "阴性预测值": "阴性预测值(95%CI)",
                "F1分数": "F1分数(95%CI)",
                "Kappa": "Kappa(95%CI)",
            },
            inplace=True,
        )
        df_test_result.rename(
            columns={
                "cutoff": "cutoff(95%CI)",
                "准确度": "准确度(95%CI)",
                "灵敏度": "灵敏度(95%CI)",
                "特异度": "特异度(95%CI)",
                "阳性预测值": "阳性预测值(95%CI)",
                "阴性预测值": "阴性预测值(95%CI)",
                "F1分数": "F1分数(95%CI)",
                "Kappa": "Kappa(95%CI)",
            },
            inplace=True,
        )
    df_dict = {
        "多模型分类-训练集结果汇总": df_train_result.drop(["AUC"], axis=1),
        "多模型分类-验证集结果汇总": df_test_result.drop(["AUC"], axis=1),
    }
    if delong:
        for ii in range(delong_zz.shape[0]):
            delong_zz.iloc[ii, ii + 1] = "NA"
        for ii in range(delong_pp.shape[0]):
            delong_pp.iloc[ii, ii + 1] = "NA"
        df_dict.update({"delong检测Z值均值表": delong_zz})
        df_dict.update({"delong检测P值均值表": delong_pp})

    if boostrap != 1:
        plot_name_dict = {
            "训练集ROC曲线图": plot_name_list[0],
            "验证集ROC曲线图": plot_name_list[1],
            "验证集多模型森林图": plot_name_list[2],
            "验证集多模型校准曲线": plot_name_list[3],
            "验证集DCA曲线图": plot_name_list[4],
        }
    else:
        plot_name_dict = {
            "训练集ROC曲线图": plot_name_list[0],
            "验证集ROC曲线图": plot_name_list[1],
            "验证集多模型校准曲线": plot_name_list[2],
            "验证集DCA曲线图": plot_name_list[3],
        }

    ###画PR曲线

    if PR:
        from sklearn.metrics import plot_precision_recall_curve
        from AnalysisFunction.X_5_SmartPlot import plot_precision_recall_curve


        fig = plot_precision_recall_curve(
            model_train_s,
            X_train_ps,
            Y_train_ps,
            name=name,
            picname="PR Curve(Training)",
            resultType=resultType,
        )
        plot_name_dict["训练集多模型PR曲线"] = save_fig(
            savePath, "PR_train", "png", fig, str_time=str_time
        )
        plot_name_dict_save["训练集多模型PR曲线"] = save_fig(
            savePath, "PR_train", picFormat, fig, str_time=str_time
        )
        plt.close(fig)
        fig = plot_precision_recall_curve(
            model_train_s,
            X_test_ps,
            Y_test_ps,
            name=name,
            picname="PR Curve(Validation)",
            resultType=resultType,
        )
        plot_name_dict["验证集多模型PR曲线"] = save_fig(
            savePath, "PR_valid", "png", fig, str_time=str_time
        )
        plot_name_dict_save["验证集多模型PR曲线"] = save_fig(
            savePath, "PR_valid", picFormat, fig, str_time=str_time
        )
        plt.close(fig)
    result_dict = {
        "str_result": {"分析结果描述": str_result},
        "tables": df_dict,
        "pics": plot_name_dict,
        "save_pics": plot_name_dict_save,
    }
    return result_dict


10.使用上面机器学习二分类的代码即可


